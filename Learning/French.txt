Il existe plusieurs approches statistiques d'identification de la langue utilisant différentes techniques pour classifier les données. Une technique consiste à comparer la compressibilité du texte à la compressibilité de textes dans un ensemble de langues connues. Cette approche est connue sous le nom de mesure de distance basée sur des informations mutuelles. La même technique peut également être utilisée pour construire de manière empirique des arbres généalogiques de langues qui correspondent étroitement aux arbres construits à l'aide de méthodes historiques. [Citation nécessaire] La mesure de distance basée sur des informations mutuelles est essentiellement équivalente à des méthodes plus classiques basées sur un modèle et n'est généralement pas considérée être nouveau ou meilleur que des techniques plus simples.

Une autre technique, décrite par Cavnar et Trenkle (1994) et Dunning (1994), consiste à créer un modèle de langage n-gramme à partir d'un "texte de formation" pour chacune des langues. Ces modèles peuvent être basés sur des caractères (Cavnar et Trenkle) ou des octets codés (Dunning); dans ce dernier cas, l'identification de la langue et la détection du codage de caractères sont intégrées. Ensuite, pour tout élément de texte devant être identifié, un modèle similaire est créé et ce modèle est comparé à chaque modèle de langage stocké. La langue la plus probable est celle avec le modèle qui ressemble le plus au modèle du texte à identifier. Cette approche peut être problématique lorsque le texte saisi est dans une langue pour laquelle il n'y a pas de modèle. Dans ce cas, la méthode peut renvoyer un autre langage, "le plus similaire", en tant que résultat. Une partie du texte d’entrée composée de plusieurs langues, comme c’est courant sur le Web, est également problématique pour toute approche.

Pour une méthode plus récente, voir et (2009). Cette méthode peut détecter plusieurs langues dans un texte non structuré et fonctionne de manière robuste sur des textes courts de quelques mots seulement: un problème avec lequel les approches de n-gramme peinent.

Une méthode statistique plus ancienne de Grefenstette était basée sur la prévalence de certains mots de fonction (par exemple, "the" en anglais).

L'un des grands goulots d'étranglement des systèmes d'identification des langues est la distinction entre les langues étroitement liées. Des langues similaires comme le serbe et le croate ou l'indonésien et le malais présentent d'importants chevauchements lexicaux et structurels, ce qui rend difficile pour les systèmes de les différencier.

Récemment, la tâche partagée DSL [1] a été organisée en fournissant un ensemble de données (Tan et al., 2014) contenant 13 langues différentes (et variétés de langues) dans six groupes linguistiques: groupe A (bosniaque, croate, serbe), groupe B ( Indonésien, malaisien), groupe C (tchèque, slovaque), groupe D (portugais brésilien, portugais européen), groupe E (Espagne péninsulaire, espagnol argentin), groupe F (anglais américain, anglais britannique). Le meilleur système a obtenu des résultats supérieurs à 95% (Goutte et al., 2014). Les résultats de la tâche partagée DSL sont décrits dans Zampieri et al. 2014.

L’histoire du traitement du langage naturel a généralement commencé dans les années 50, mais des travaux datant de périodes plus anciennes peuvent être trouvés. En 1950, Alan Turing publia un article intitulé "Intelligence" qui proposait ce que l'on appelle maintenant le test de Turing comme critère de l'intelligence.

L’expérience de Georgetown en 1954 impliquait la traduction entièrement automatique de plus de soixante phrases en russe en anglais. Les auteurs ont affirmé que d'ici trois ou cinq ans, la traduction automatique serait un problème résolu [2]. Cependant, les progrès réels ont été beaucoup plus lents et, après le rapport ALPAC de 1966, qui concluait que dix années de recherche n’avaient pas répondu aux attentes, le financement de la traduction automatique avait été considérablement réduit. Peu de recherches supplémentaires sur la traduction automatique ont été menées jusqu'à la fin des années 1980, lorsque les premiers systèmes de traduction automatique statistiques ont été mis au point.

SHRDLU, un système de langage naturel fonctionnant dans des "mondes de blocs" restreints avec des vocabulaires restreints, et ELIZA, une simulation d'un psychothérapeute rogérien, écrit par Joseph Weizenbaum entre 1964 et 1966. En l'absence d'informations sur la pensée ou les émotions humaines, ELIZA offrait parfois une interaction étonnamment humaine. Lorsque le "patient" dépasse la très petite base de connaissances, ELIZA peut fournir une réponse générique, par exemple en répondant à "Ma tête me fait mal" avec "Pourquoi dites-vous que votre tête vous fait mal?".

Au cours des années 1970, de nombreux programmeurs ont commencé à écrire des "ontologies conceptuelles", qui structuraient les informations du monde réel en données compréhensibles par ordinateur. MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politique (Carbonell, 1979) et Unités de parcelles de terrain (Lehnert, 1981). ). Pendant ce temps, de nombreux chatterbots ont été écrits, y compris PARRY, Racter et Jabberwacky.

Jusque dans les années 1980, la plupart des systèmes de traitement du langage naturel reposaient sur des ensembles complexes de règles manuscrites. À partir de la fin des années 1980, le traitement du langage naturel a toutefois révolutionné l'introduction d'algorithmes d'apprentissage automatique pour le traitement du langage. Cela était dû à la fois à l'augmentation constante du pouvoir de calcul (voir la loi de Moore) et à la diminution progressive de la domination des théories linguistiques de Chomskya (par exemple, la grammaire transformationnelle), dont les fondements théoriques décourageaient le type de linguistique de corpus qui sous-tendait la méthode d'apprentissage par la machine. au traitement de la langue. [3] Certains des premiers algorithmes d’apprentissage automatique, tels que les arbres de décision, ont produit des systèmes de règles dures si-alors similaires aux règles écrites à la main. Cependant, le marquage partiel a introduit l’utilisation de modèles de Markov cachés dans le traitement du langage naturel, et de plus en plus de recherches ont été consacrées aux modèles statistiques, qui permettent de prendre des décisions souples et probabilistes fondées sur l’attribution de pondérations réelles aux caractéristiques constituant l’entrée. Les données. Les modèles de langage de cache sur lesquels de nombreux systèmes de reconnaissance vocale reposent maintenant sont des exemples de tels modèles statistiques. De tels modèles sont généralement plus robustes lorsqu'ils reçoivent des entrées inconnues, en particulier des entrées contenant des erreurs (comme cela est très courant pour les données du monde réel), et produisent des résultats plus fiables lorsqu'ils sont intégrés dans un système plus vaste comprenant plusieurs sous-tâches.

Un grand nombre des premiers succès notables ont été enregistrés dans le domaine de la traduction automatique, en particulier grâce au travail effectué par IBM Research, où des modèles statistiques de plus en plus complexes ont été développés. Ces systèmes ont pu tirer parti des corpus textuels multilingues existants produits par le Parlement du Canada et l'Union européenne à la suite de lois demandant la traduction de toutes les procédures gouvernementales dans toutes les langues officielles des systèmes de gouvernement correspondants. Cependant, la plupart des autres systèmes reposaient sur des corpus développés spécifiquement pour les tâches mises en œuvre par ces systèmes, ce qui constituait (et continue souvent d’être) une limitation majeure du succès de ces systèmes. En conséquence, de nombreuses recherches ont été consacrées aux méthodes d’apprentissage plus efficace à partir de quantités limitées de données.

Les recherches récentes se sont de plus en plus concentrées sur des algorithmes d'apprentissage non supervisés et semi-supervisés. Ces algorithmes peuvent apprendre des données qui n'ont pas été annotées à la main avec les réponses souhaitées, ou en utilisant une combinaison de données annotées et non annotées. En règle générale, cette tâche est beaucoup plus difficile que l’apprentissage supervisé et produit généralement des résultats moins précis pour une quantité donnée de données d’entrée. Cependant, il existe une énorme quantité de données non annotées disponibles (y compris, entre autres, tout le contenu du World Wide Web), qui peuvent souvent compenser les résultats inférieurs si l'algorithme utilisé a une complexité temporelle suffisamment faible pour soit pratique.

Dans les années 2010, les méthodes d'apprentissage par la représentation et d'apprentissage par machine à la manière d'un réseau de neurones profonds se sont généralisées dans le traitement du langage naturel, en partie à cause d'une multitude de résultats montrant que de telles techniques [4] [5] peuvent donner des résultats à la pointe de la technologie. dans de nombreuses tâches en langage naturel, par exemple dans la modélisation du langage, l'analyse syntaxique [6], [7] [8] et bien d'autres. Les techniques populaires incluent l’utilisation d’incorporations de mots pour capturer les propriétés sémantiques des mots, ainsi que l’augmentation de l’apprentissage de bout en bout d’une tâche de niveau supérieur (par exemple, la réponse à une question) au lieu de compter sur un ensemble de tâches intermédiaires distinctes (par exemple, marquage partiel et analyse de dépendance). Dans certains domaines, ce changement a entraîné des changements substantiels dans la conception des systèmes de la PNL, de sorte que les approches basées sur des réseaux neuronaux profonds peuvent être considérées comme un nouveau paradigme distinct du traitement statistique du langage naturel. Par exemple, le terme traduction neuronale (NMT) souligne le fait que les approches approfondies de la traduction automatique apprises en profondeur apprennent directement les transformations séquence à séquence, évitant ainsi la nécessité d’étapes intermédiaires telles que l’alignement des mots et la modélisation du langage qui ont été utilisées dans la modélisation statistique. traduction automatique (SMT).

Au début, de nombreux systèmes de traitement du langage ont été conçus en codant à la main un ensemble de règles [9] [10], par ex. en écrivant des grammaires ou en élaborant des règles heuristiques pour la radicalisation. Cependant, cela est rarement robuste aux variations du langage naturel.

Depuis la prétendue "révolution statistique" [11] [12] à la fin des années 1980 et au milieu des années 1990, une grande partie de la recherche sur le traitement du langage naturel s'est largement appuyée sur l'apprentissage automatique.

Le paradigme de l'apprentissage automatique appelle plutôt l'utilisation de l'inférence statistique pour apprendre automatiquement de telles règles par l'analyse de grands corpus d'exemples typiques du monde réel (un corpus (pluriel, "corpus") est un ensemble de documents, éventuellement avec des annotations humaines ou informatiques. ).

De nombreuses classes différentes d'algorithmes d'apprentissage automatique ont été appliquées aux tâches de traitement en langage naturel. Ces algorithmes prennent en entrée un grand ensemble de "fonctionnalités" générées à partir des données en entrée. Certains des premiers algorithmes utilisés, tels que les arbres de décision, produisaient des systèmes de règles dures si-alors similaires aux systèmes de règles manuscrites qui étaient alors courants. Cependant, la recherche a de plus en plus mis l'accent sur des modèles statistiques, qui permettent de prendre des décisions aléatoires et probabilistes basées sur l'attribution de pondérations réelles à chaque entité en entrée. Ces modèles ont l'avantage de pouvoir exprimer la certitude relative de nombreuses réponses possibles différentes plutôt que d'une seule, produisant des résultats plus fiables lorsqu'un tel modèle est inclus en tant que composant d'un système plus vaste.

Les systèmes basés sur des algorithmes d’apprentissage automatique présentent de nombreux avantages par rapport aux règles créées manuellement:

Les procédures d’apprentissage utilisées lors de l’apprentissage automatique se concentrent automatiquement sur les cas les plus courants. Tandis que lorsqu’il s’agit de rédiger des règles à la main, il n’est souvent pas évident de diriger l’effort.

Les procédures d’apprentissage automatique peuvent utiliser des algorithmes d’inférence statistique pour produire des modèles qui résistent aux entrées inconnues (contenant par exemple des mots ou des structures qui n’ont pas été vus auparavant) et aux entrées erronées (avec des mots mal orthographiés ou omis accidentellement). En règle générale, gérer ces entrées avec des règles manuscrites ou, plus généralement, créer des systèmes de règles manuscrites qui permettent de prendre des décisions en douceur, est extrêmement difficile, source d'erreurs et prend beaucoup de temps.

Les systèmes basés sur l'apprentissage automatique des règles peuvent être rendus plus précis simplement en fournissant davantage de données d'entrée. Cependant, les systèmes basés sur des règles manuscrites ne peuvent être rendus plus précis qu'en augmentant leur complexité, ce qui est une tâche beaucoup plus difficile. En particulier, il existe une limite à la complexité des systèmes basés sur des règles conçues à la main, au-delà desquelles les systèmes deviennent de plus en plus ingérables. Cependant, la création de davantage de données à saisir dans les systèmes d’apprentissage automatique nécessite simplement une augmentation correspondante du nombre d’heures-homme travaillées, généralement sans augmentation significative de la complexité du processus d’annotation.

La détection du codage de caractères, la détection du jeu de caractères ou la détection de la page de code est le processus consistant à deviner de manière heuristique le codage de caractères d'une série d'octets représentant du texte. La technique est reconnue comme non fiable et n'est utilisée que lorsque des métadonnées spécifiques, telles qu'un en-tête HTTP Content-Type: ne sont pas disponibles ou sont considérées comme non fiables.

Cet algorithme implique généralement une analyse statistique des structures d’octets, telles que la distribution de fréquence des trigraphes de différentes langues codées dans chaque page de code à détecter. une telle analyse statistique peut également être utilisée pour effectuer une détection de langue. Ce processus n’est pas infaillible car il dépend de données statistiques.

En général, une détection de jeu de caractères incorrecte conduit à mojibake.

L'un des rares cas où la détection de jeu de caractères fonctionne de manière fiable est la détection de UTF-8. Cela est dû au pourcentage élevé de séquences d'octets non valides dans UTF-8. Par conséquent, il est extrêmement improbable qu'un texte d'un autre codage utilisant des octets dont le bit de poids fort est défini réussisse un test de validité UTF-8. Cependant, les routines de détection de jeu de caractères mal écrites n'exécutent pas le test UTF-8 fiable en premier et peuvent décider qu'UTF-8 est un autre codage. Par exemple, il était courant que les sites Web au format UTF-8 contenant le nom de la ville allemande de Munich soient présentés comme étant München.

UTF-16 est relativement fiable à détecter en raison du nombre élevé de nouvelles lignes (U + 000A) et d'espaces (U + 0020) qui doivent être trouvés lors de la division des données en mots de 16 bits, et du fait que peu d'encodages utilisent 16- peu de mots. Ce processus n'est pas infaillible; Par exemple, certaines versions du système d’exploitation Windows détectent mal la phrase "Bush cache les faits" (sans nouvelle ligne) en ASCII en chinois UTF-16LE.

La détection de jeu de caractères est particulièrement peu fiable en Europe, dans un environnement de codages mixtes ISO-8859. Ce sont des codages à huit bits étroitement liés qui partagent un chevauchement dans leur moitié inférieure avec ASCII. Il n’existe aucun moyen technique de différencier ces codages et leur reconnaissance repose sur l’identification de fonctions linguistiques, telles que la fréquence des lettres ou l’orthographe.

En raison du manque de fiabilité de la détection heuristique, il est préférable d'étiqueter correctement les jeux de données avec le bon codage. Les documents HTML servis sur le Web par HTTP doivent avoir leur codage déclaré hors bande à l'aide de l'en-tête Content-Type:.

Ce domaine constituera un axe de recherche et d’innovation dans les domaines liés aux systèmes de sécurité et aux enquêtes judiciaires et s’appuiera sur la réputation nationale et internationale des compétences existantes dans ces domaines à l’Université de Canberra. Il réunira des chercheurs clés des trois divisions de l'université, établira des liens avec les domaines de recherche existants dans les différentes disciplines de l'université de Canberra et collaborera avec des organisations externes liées à ce domaine. Ce centre traitera des problèmes clés et étendra les résultats potentiels dans cet important domaine en réunissant un groupe multidisciplinaire de chercheurs et de professionnels qui collaboreront étroitement avec le gouvernement, le secteur privé et les professions pour obtenir les résultats escomptés.

Être un centre d’excellence reconnu au niveau national et international dans le domaine de la recherche, qui propose des solutions multidisciplinaires complètes et intégrées aux problèmes auxquels sont confrontés les domaines de la sécurité et de la criminalistique et qui suscite des collaborations novatrices avec l’industrie et des professionnels dont les résultats présentent des avantages directs pour les entreprises. les communautés nationales et internationales

Développer une approche globale de la recherche axée sur l'amélioration de la sécurité dans un éventail de domaines qui produira des résultats nouveaux, intégrés et synergiques dans les domaines des sciences de l'information, de la cryptologie, des systèmes électroniques et du crime électronique, de la protection des frontières et des systèmes biologiques, y compris l'environnement, la santé animale et humaine.
Identifier et coordonner des programmes de recherche innovants dans les domaines de la sécurité et de la criminalistique qui impliquent le secteur de l'industrie et offrent des collaborations de recherche en synergie avec l'industrie et des programmes d'étudiants de troisième cycle;
D'être proactif dans l'identification des domaines nécessitant une avancée stratégique des connaissances et des solutions aux menaces existantes et nouvelles en matière de sécurité et aux évaluations médico-légales, et diffuser les résultats de la mise en œuvre dans les secteurs industriels publics ou privés concernés; et
Offrir des avantages réels et futurs dans les domaines de la sécurité et de la criminalistique grâce à la participation active des spécialistes des questions juridiques et politiques, ce qui entraînera une réduction de la criminalité grâce à une analyse améliorée des preuves relatives aux systèmes de condamnation et de prévention.

Ces objectifs seront mesurés par les résultats et les produits résultant de la recherche et de leur utilisation par l'industrie et le secteur professionnel, par leur influence sur la société, par leurs effets sur les politiques et par les avantages mesurables qu'ils procurent à l'application de la loi et aux procédures judiciaires.

La dernière décennie a été témoin de profonds changements dans nos exigences en matière de sécurité nationale et dans la structure des conflits humains. En conséquence, les gouvernements exigent des solutions intégrées, coordonnées et rentables pour répondre à un spectre de menaces élargi. Il existe un besoin manifeste de facilitation similaire en matière de sécurité et de recherche médico-légale en Australie. Ce centre est unique en ce sens qu'il est conçu pour traiter une analyse interdisciplinaire plus large des besoins de l'Australie en matière de sécurité et de criminalistique et qu'il adopte une approche stratégique plus globale des problèmes importants. Le centre proposé vise à identifier et à faciliter des programmes de recherche qui amélioreront la sécurité des Australiens en améliorant la sensibilisation, l’intégration et la coordination des ressources et stratégies existantes pour l’élaboration de politiques, de capacités et de procédures de protection et de lutte contre le crime australiens.

Ce centre réunira une combinaison unique de chercheurs professionnels et d'utilisateurs finaux dans les disciplines de la médecine légale; sciences de l'information, biométrie et communications sécurisées; sciences médicales, sanitaires et environnementales; application de la loi, douanes et protection des frontières; sécurité électronique, décodage et fraude; gouvernance et gestion du secteur public; droit et preuves; et modélisation sociale et économique. Le Centre établira des liens essentiels et uniques entre ces groupes pour permettre des synergies de recherche, de politiques et opérationnelles qui ne se produiraient pas autrement. Ensemble, ils feront progresser la recherche en matière de sécurité aux niveaux national et international et élaboreront des options novatrices pour mieux anticiper et gérer les menaces potentiellement graves à la santé, au bien-être et à la sécurité des Australiens. Les recherches effectuées par le Centre apportent d’autres avantages aux retombées sur la coopération entre portefeuilles et à des résultats susceptibles de conseiller et d’orienter les gouvernements et l’industrie.

La recherche en matière de sécurité est axée sur le maintien de la viabilité et la protection de divers systèmes contre les menaces et dépend de la contribution de nombreuses disciplines travaillant ensemble pour garantir la mise en place d'un système prenant en compte tous les facteurs pertinents. La protection intervient à plusieurs niveaux stratégiques et comprend la compréhension de la nature d'une menace, la préparation à une menace potentielle, la prévention des conséquences d'une menace, la réponse aux conséquences d'une menace et le rétablissement à la suite des conséquences d'une menace. Pour aborder ces niveaux stratégiques, il est essentiel d’intégrer des recherches qui traitent des questions de criminalistique et de droit afin de permettre une intervention et un rétablissement efficaces. Les menaces à la sécurité peuvent être involontaires, accidentelles et non organisées, ou bien délibérées, organisées et malveillantes (y compris des activités criminelles et terroristes). Bien que ces dernières menaces occupent une place de choix dans les médias, l’ancienne classe de menaces à la sécurité peut avoir des conséquences sociales, environnementales et économiques dévastatrices. À titre d'exemple, on estime maintenant que le vol d'identité coûte plus de 1,2 milliard de dollars par an à l'Australie par carte de crédit, compte bancaire, sécurité sociale et autres types de vol frauduleux.

À l'Université de Canberra, nous avons des universitaires, du personnel de recherche et des étudiants possédant l'expertise, la base de discipline, les ressources, les liens de collaboration et le profil de recherche pour atteindre les objectifs de ce domaine de recherche. Ce centre est unique en ce sens qu’il s’agit d’un domaine multidisciplinaire intégré de force de recherche «inter-université» axé sur la priorité de recherche nationale de la sauvegarde de l’Australie. Notre implantation dans la capitale nationale fait également de nous un «naturel» de jouer un rôle de réseautage et de rapprochement réunissant des chercheurs, des professions, l’industrie et le gouvernement afin d’élaborer des programmes et d’obtenir des résultats conformes aux priorités nationales. Outre ses ressources spécialisées, l'UC possède également certaines infrastructures (par exemple, une salle d'audience électronique, des laboratoires de normalisation ISO, etc.) qui nous offrent un avantage concurrentiel unique par rapport à d'autres universités.

Le diplôme en langues est disponible pour tous les étudiants inscrits au premier cycle de l’Université. Ce diplôme permet aux étudiants de compléter une séquence de trois ans dans une langue étrangère tout en maintenant les exigences du diplôme choisi. Il est disponible pour les étudiants débutants et pour les étudiants ayant une connaissance préalable de la langue aux niveaux Continu et Intermédiaire-Avancé (voir entrées séparées). Les étudiants s'inscrivent dans la langue de leur choix à chaque année et sont normalement censés s'inscrire moins d'une charge à temps plein dans leurs unités de diplômes pour cette année. Ce processus prolongera la période nécessaire pour terminer les deux programmes à un minimum de quatre ans. Les étudiants peuvent commencer les unités de diplôme en première ou deuxième année de leur diplôme de premier cycle. Les étudiants qui commencent le diplôme en troisième année d'études de premier cycle peuvent obtenir leur diplôme et terminer les unités restantes du diplôme à temps partiel.

Ce diplôme permet aux étudiants de développer des compétences linguistiques et des compétences culturelles en chinois, japonais ou espagnol, en ajoutant de la valeur et en internationalisant leur diplôme. Les étudiants acquerront des compétences linguistiques qui pourront être utilisées dans le cadre de leurs carrières professionnelles afin d’accroître leurs possibilités d’emploi en Australie ou à l’étranger. À la fin du cours, les étudiants auront acquis un niveau intermédiaire de compétences en communication et de compétences culturelles dans la langue de leur choix.

Un étudiant à temps plein étudie normalement quatre unités au cours de chacun des trois semestres, mais peut prolonger le cours à deux ans en étudiant trois unités par semestre. Un étudiant à temps partiel étudie normalement deux unités par semestre. Au cours du premier semestre, les étudiants doivent suivre la pratique professionnelle G. Les trois autres unités peuvent être choisies parmi diverses unités de premier cycle (G) ou de troisième cycle (PG) dans les domaines de la programmation, de l'analyse et de la conception de systèmes, des réseaux informatiques et des projets. et gestion de la qualité. Les unités de niveau G seraient normalement sélectionnées pour satisfaire à toutes les conditions préalables requises pour que les unités de niveau PG soient prises aux semestres 2 et 3.

Aux deuxième et troisième semestres, les étudiants prennent des unités de niveau PG dans les domaines qui les intéressent. Les unités de niveau PG actuellement proposées sont répertoriées ci-dessous, regroupées par spécialisation. Les unités suivies d’un astérisque (*) doivent avoir au moins deux ans d’expérience professionnelle. Veuillez noter que chaque année, seule une sélection de ces unités fonctionnera.
En plus de la liste ci-dessous, des unités de projet individuelles et (pour les étudiants spécialisés en informatique de gestion) un stage sont disponibles

Les acronymes de nouvelles sont des acronymes de frases utilisés dans la rédaction de nouvelles. Ils sont principalement composés des premières lettres des mots en frase et sont mieux écrits en majuscules. Les frases sont généralement en anglais, bien que certains puissent être en langues maternelles.

La reconnaissance des formes est généralement classée en fonction du type de procédure d'apprentissage utilisée pour générer la valeur de sortie. L'apprentissage supervisé suppose qu'un ensemble de données d'apprentissage (l'ensemble d'apprentissage) a été fourni et consiste en un ensemble d'instances correctement étiquetées à la main avec le résultat correct. Les modèles de formation sont les objectifs du processus de formation et ne doivent pas être confondus avec l'ensemble de formation. [7] Une procédure d’apprentissage génère ensuite un modèle qui tente de répondre à deux objectifs parfois contradictoires: Tirer le meilleur parti possible des données d’apprentissage et généraliser au mieux aux nouvelles données (en général, cela signifie qu’il est aussi simple que possible, pour une définition technique; "simple", conformément au rasoir d'Occam, discuté ci-dessous). D'autre part, l'apprentissage non supervisé suppose des données d'apprentissage qui n'ont pas été étiquetées à la main et tente de trouver des modèles inhérents aux données qui peuvent ensuite être utilisés pour déterminer la valeur de sortie correcte pour les nouvelles instances de données. [8] L'apprentissage semi-supervisé combine les données étiquetées et non étiquetées (généralement un petit ensemble de données étiquetées associé à une grande quantité de données non étiquetées). Notez que dans les cas d'apprentissage non supervisé, il peut ne pas y avoir de données de formation à proprement parler. en d'autres termes, et les données à étiqueter sont les données d'apprentissage.

Notez que des termes parfois différents sont utilisés pour décrire les procédures d’apprentissage supervisées et non supervisées correspondantes pour le même type de sortie. Par exemple, l’équivalent non supervisé de la classification est généralement appelé clustering, basé sur la perception commune que la tâche ne nécessite aucune donnée d’apprentissage, et sur le regroupement des données d’entrée en clusters basés sur une mesure de similarité inhérente (par exemple, la distance entre instances, considérées comme des vecteurs dans un espace vectoriel multidimensionnel), plutôt que d'affecter chaque instance d'entrée à l'une des classes prédéfinies. Notez également que dans certains domaines, la terminologie est différente: par exemple, dans l'écologie de la communauté, le terme "classification" est utilisé pour désigner ce que l'on appelle communément "clustering".

La donnée d'entrée pour laquelle une valeur de sortie est générée est officiellement appelée instance. L'instance est formellement décrite par un vecteur de caractéristiques, qui constituent ensemble une description de toutes les caractéristiques connues de l'instance. (Ces vecteurs de caractéristiques peuvent être vus comme définissant des points dans un espace multidimensionnel approprié, et des méthodes de manipulation de vecteurs dans des espaces de vecteurs peuvent leur être appliquées en conséquence, telles que le calcul du produit scalaire ou de l'angle entre deux vecteurs.) En règle générale, les caractéristiques sont soit catégorique (également appelé nominal, c’est-à-dire constitué d’un ensemble d’articles non ordonnés, tels que le sexe de "homme" ou "femme", ou un groupe sanguin de "A", "B", "AB" ou " O "), ordinale (consistant en un ensemble d'éléments ordonnés, par exemple" grand "," moyen "ou" petit "), à valeur entière (par exemple, un compte du nombre d'occurrences d'un mot particulier dans une courriel) ou à valeur réelle (par exemple, une mesure de la pression artérielle). Souvent, les données catégorielles et ordinales sont regroupées; de même pour les données à valeurs entières et à valeurs réelles. En outre, de nombreux algorithmes fonctionnent uniquement en termes de données catégorielles et exigent que les données à valeurs réelles ou entières soient discrétisées en groupes (par exemple, moins de 5, entre 5 et 10, ou plus de 10).

De nombreux algorithmes de reconnaissance de modèle courants sont de nature probabiliste, en ce sens qu'ils utilisent l'inférence statistique pour trouver la meilleure étiquette pour une instance donnée. Contrairement aux autres algorithmes, qui produisent simplement une "meilleure" étiquette, les algorithmes probabilistes fournissent également une probabilité que l'instance soit décrite par l'étiquette donnée. En outre, de nombreux algorithmes probabilistes produisent une liste des N meilleures étiquettes avec les probabilités associées, pour une valeur de N au lieu d'une simple meilleure étiquette. Lorsque le nombre d'étiquettes possibles est assez petit (par exemple, dans le cas d'une classification), N peut être défini de sorte que la probabilité de toutes les étiquettes possibles soit générée. Les algorithmes probabilistes présentent de nombreux avantages par rapport aux algorithmes non probabilistes:

Ils produisent une valeur de confiance associée à leur choix. (Notez que certains autres algorithmes peuvent également générer des valeurs de confiance, mais en général, cette valeur n’est mathématiquement fondée sur la théorie des probabilités que pour les algorithmes probabilistes. En règle générale, les valeurs de confiance non probabilistes ne peuvent donner aucune signification particulière et ne peuvent être comparées à autres valeurs de confiance émises par le même algorithme.)
De manière correspondante, ils peuvent s'abstenir lorsque la confiance de choisir une production particulière est trop faible.
En raison des probabilités générées, les algorithmes probabilistes de reconnaissance de formes peuvent être intégrés plus efficacement à des tâches plus complexes d’apprentissage automatique, de manière à éviter partiellement ou totalement le problème de la propagation des erreurs.

Les algorithmes de sélection de caractéristiques tentent d'éliminer directement les caractéristiques redondantes ou non pertinentes. Une introduction générale à la sélection des fonctionnalités, résumant les approches et les défis, a été donnée. [9] La complexité de la sélection des fonctionnalités est, en raison de son caractère non monotone, un problème d'optimisation où un total de {\ displaystyle n} n présente le jeu de pouvoir constitué de tous les {\ displaystyle 2 ^ {n} -1} 2 ^ { n} -1 sous-ensembles de fonctionnalités doivent être explorés. L’algorithme Branch-and-Bound [10] réduit cette complexité, mais il est difficile à traiter pour les valeurs moyennes à grandes du nombre de fonctionnalités disponibles {\ displaystyle n} n. Pour une comparaison à grande échelle des algorithmes de sélection de caractéristiques, voir [11].

Des techniques permettant de transformer les vecteurs de caractéristiques bruts (extraction de caractéristiques) sont parfois utilisées avant l’application de l’algorithme de filtrage. Par exemple, les algorithmes d'extraction de caractéristiques tentent de réduire un vecteur de caractéristiques de grande dimensionnalité en un vecteur de dimensionnalité plus petite, plus facile à manipuler et codant moins de redondance, à l'aide de techniques mathématiques telles que l'analyse en composantes principales (ACP). La distinction entre la sélection et l'extraction d'entités est que les entités résultantes après l'extraction sont d'un type différent de celles d'origine et peuvent ne pas être facilement interprétables, tandis que les entités laissées après la sélection d'entités ne sont qu'un sous-ensemble des entités d'origine. .

Le traitement du langage naturel [110] (NLP) donne aux machines la capacité de lire et de comprendre le langage humain. Un système de traitement du langage naturel suffisamment puissant permettrait des interfaces utilisateur en langage naturel et l'acquisition de connaissances directement à partir de sources écrites par l'homme, telles que les textes de fil de presse. Certaines applications simples du traitement en langage naturel incluent la récupération d'informations, l'exploration de texte, la réponse à une question [111] et la traduction automatique. [112] De nombreuses approches actuelles utilisent des fréquences de cooccurrence de mots pour construire des représentations syntaxiques de texte. Les stratégies de "recherche de mots clés" pour la recherche sont populaires et évolutives, mais stupides; une requête de recherche pour "chien" peut uniquement associer des documents contenant le mot littéral "chien" et manquer un document contenant le mot "caniche". Les stratégies "d'affinité lexicale" utilisent l'occurrence de mots tels que "accident" pour évaluer le sentiment d'un document. Les approches de PNL statistiques modernes peuvent combiner toutes ces stratégies ainsi que d’autres, et permettent souvent d’obtenir une précision acceptable au niveau de la page ou du paragraphe, mais ne disposent toujours pas de la compréhension sémantique nécessaire pour bien classifier les phrases isolées. Besides the usual difficulties with encoding semantic commonsense knowledge, existing semantic NLP sometimes scales too poorly to be viable in business applications. Beyond semantic NLP, the ultimate goal of "narrative" NLP is to embody a full understanding of commonsense reasoning.[113]

Outre les difficultés habituelles liées au codage des connaissances sémantiques de bon sens, la PNL sémantique existante est parfois trop médiocre pour être viable dans les applications métiers. Au-delà de la PNL sémantique, le but ultime de la PNL "narrative" est d'incarner une compréhension complète du raisonnement de bon sens. [113]
La perception de la machine [114] est la capacité à utiliser les entrées de capteurs (tels que des caméras (spectre visible ou infrarouge), des microphones, des signaux sans fil et des capteurs lidar, sonar, radar et tactiles actifs) pour déduire des aspects du monde. Les applications comprennent la reconnaissance de la parole, la reconnaissance faciale et la reconnaissance d'objet. [116] La vision par ordinateur est la capacité d'analyser les entrées visuelles. Une telle entrée est généralement ambiguë; Un piéton géant haut de cinquante mètres éloigné peut produire exactement les mêmes pixels qu’un piéton de taille normale à proximité, ce qui oblige l’intéressé à juger de la probabilité relative et du caractère raisonnable de différentes interprétations, en utilisant par exemple son "modèle objet" que les piétons de cinquante mètres n'existent pas. [117]

L'apprentissage automatique, concept fondamental de la recherche sur l'IA depuis la création de ce domaine [104], est l'étude d'algorithmes informatiques qui s'améliorent automatiquement grâce à l'expérience. [105] [106]

L'apprentissage non supervisé est la capacité de trouver des modèles dans un flux d'entrées, sans qu'il soit nécessaire qu'un humain commence par étiqueter les entrées. [107] L'apprentissage supervisé comprend à la fois une classification et une régression numérique, ce qui nécessite un humain pour étiqueter les données d'entrée en premier. La classification est utilisée pour déterminer la catégorie à laquelle appartient une chose, après avoir vu un certain nombre d'exemples d'éléments appartenant à plusieurs catégories. La régression est la tentative de produire une fonction qui décrit la relation entre les entrées et les sorties et prédit comment les sorties devraient changer en même temps que les entrées. [106] Les classificateurs et les apprenants de régression peuvent être considérés comme des "approximateurs de fonction" essayant d'apprendre une fonction inconnue (éventuellement implicite); Par exemple, un classificateur de courrier indésirable peut être considéré comme une fonction qui associe le texte d'un e-mail à l'une des deux catégories "spam" ou "non spam". La théorie de l'apprentissage informatique peut évaluer les apprenants en fonction de la complexité informatique, de la complexité de l'échantillon (quantité de données requise) ou d'autres notions d'optimisation. [108] Dans l'apprentissage par renforcement [109], l'agent est récompensé pour ses bonnes réponses et puni pour ses inconvénients. L'agent utilise cette séquence de récompenses et de punitions pour former une stratégie d'exploitation dans son espace de problèmes.
