There are several statistical approaches to language identification using different techniques to classify the data. One technique is to compare the compressibility of the text to the compressibility of texts in a set of known languages. This approach is known as mutual information based distance measure. The same technique can also be used to empirically construct family trees of languages which closely correspond to the trees constructed using historical methods.[citation needed] Mutual information based distance measure is essentially equivalent to more conventional model-based methods and is not generally considered to be either novel or better than simpler techniques.

Another technique, as described by Cavnar and Trenkle (1994) and Dunning (1994) is to create a language n-gram model from a "training text" for each of the languages. These models can be based on characters (Cavnar and Trenkle) or encoded bytes (Dunning); in the latter, language identification and character encoding detection are integrated. Then, for any piece of text needing to be identified, a similar model is made, and that model is compared to each stored language model. The most likely language is the one with the model that is most similar to the model from the text needing to be identified. This approach can be problematic when the input text is in a language for which there is no model. In that case, the method may return another, "most similar" language as its result. Also problematic for any approach are pieces of input text that are composed of several languages, as is common on the Web.

For a more recent method, see and (2009). This method can detect multiple languages in an unstructured piece of text and works robustly on short texts of only a few words: something that the n-gram approaches struggle with.

An older statistical method by Grefenstette was based on the prevalence of certain function words (e.g., "the" in English).

One of the great bottlenecks of language identification systems is to distinguish between closely related languages. Similar languages like Serbian and Croatian or Indonesian and Malay present significant lexical and structural overlap, making it challenging for systems to discriminate between them.

Recently, the DSL shared task[1] has been organized providing a dataset (Tan et al., 2014) containing 13 different languages (and language varieties) in six language groups: Group A (Bosnian, Croatian, Serbian), Group B (Indonesian, Malaysian), Group C (Czech, Slovakian), Group D (Brazilian Portuguese, European Portuguese), Group E (Peninsular Spain, Argentine Spanish), Group F (American English, British English). The best system reached performance of over 95% results (Goutte et al., 2014). Results of the DSL shared task are described in Zampieri et al. 2014.

The history of natural language processing generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled "Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.

The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.

Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".

During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.

Up to the 1980s,most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.

Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.

Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.

In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[4][5] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[6] parsing,[7][8] and many others. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that were used in statistical machine translation (SMT).

In the early days, many language-processing systems were designed by hand-coding a set of rules,[9][10], e.g. by writing grammars or devising heuristic rules for stemming. However, this is rarely robust to natural language variation.

Since the so-called "statistical revolution"[11][12] in the late 1980s and mid 1990s, much natural language processing research has relied heavily on machine learning.

The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, "corpora") is a set of documents, possibly with human or computer annotations).

Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.

Systems based on machine-learning algorithms have many advantages over hand-produced rules:

The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.

Automatic learning procedures can make use of statistical-inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with hand-written rules—or, more generally, creating systems of hand-written rules that make soft decisions—is extremely difficult, error-prone and time-consuming.

Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on hand-written rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on hand-crafted rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.

Character encoding detection, charset detection, or code page detection is the process of heuristically guessing the character encoding of a series of bytes that represent text. The technique is recognised to be unreliable and is only used when specific metadata, such as a HTTP Content-Type: header is either not available, or is assumed to be untrustworthy.

This algorithm usually involves statistical analysis of byte patterns, like frequency distribution of trigraphs of various languages encoded in each code page that will be detected; such statistical analysis can also be used to perform language detection. This process is not foolproof because it depends on statistical data.

In general, incorrect charset detection leads to mojibake.

One of the few cases where charset detection works reliably is detecting UTF-8. This is due to the large percentage of invalid byte sequences in UTF-8, so that text in any other encoding that uses bytes with the high bit set is extremely unlikely to pass a UTF-8 validity test. However, badly written charset detection routines do not run the reliable UTF-8 test first, and may decide that UTF-8 is some other encoding. For example, it was common that web sites in UTF-8 containing the name of the German city München were shown as MÃ¼nchen.

UTF-16 is fairly reliable to detect due to the high number of newlines (U+000A) and spaces (U+0020) that should be found when dividing the data into 16-bit words, and the fact that few encodings use 16-bit words. This process is not foolproof; for example, some versions of the Windows operating system would mis-detect the phrase "Bush hid the facts" (without a newline) in ASCII as Chinese UTF-16LE.

Charset detection is particularly unreliable in Europe, in an environment of mixed ISO-8859 encodings. These are closely related eight-bit encodings that share an overlap in their lower half with ASCII. There is no technical way to tell these encodings apart and recognising them relies on identifying language features, such as letter frequencies or spellings.

Due to the unreliability of heuristic detection, it is better to properly label datasets with the correct encoding. HTML documents served across the web by HTTP should have their encoding stated out-of-band using the Content-Type: header.

This area will provide a focus for research and innovation in areas related to security systems and forensic investigation and will build upon the national and international standing of the existing expertise in these fields at the University of Canberra. It will bring together key researchers from across all three Divisions of the University, link with existing areas of research strength from the different disciplines at the University of Canberra and partner with external organisations related to this field. This Centre will address key issues and extend the potential outcomes within this important field through bringing together a multi-disciplinary group of researchers and professionals who will work closely with government, industry and the professions to achieve the stated outcomes

To be a nationally and internationally recognised Centre of Excellence in research that is proactive in providing comprehensive and integrated multi-disciplinary solutions to issues confronting the Security and Forensic fields and that attracts innovative collaborations with industry and professionals that have outcomes that are of direct benefit to the national and international communities

To develop a comprehensive approach to research focused on improving security across a range of areas that will have novel, integrated and synergistic outcomes in the areas of information sciences, cryptology, electronic systems and e-crime, border protection and customs, and biological systems, including the environment, animal and human health.
To identify and coordinate innovative research programs in both security and forensic fields that engage the industry sector and provide synergistic industry-linked research collaborations and post-graduate student programs;
To be proactive in identifying areas requiring strategic advancement of knowledge in and solutions to new and existing security threats and forensic assessments and disseminate the outcomes for implementation within the relevant public or private industry sectors; and
To provide actual and future benefits in the security and forensic fields through active engagement of the legal and policy specialists that will result in a reduction in crime through improved evidence analysis for conviction and prevention systems.

These goals will be measured by the outcomes and outputs resulting from the research and the utilisation of these within the industry and professional sector, the influence they have on society, on affecting policy and through the measurable benefit they provide to law enforcement and legal procedure

The last decade has witnessed major changes to both our national security requirements and in the patterns of human conflict.  Consequently, governments demand integrated, coordinated and cost-effective solutions for responding to a broadened threat spectrum. There is a clear need for similar Security and Forensic research facilitation in Australia. This Centre is unique in that it is designed to deal with a broader interdisciplinary analysis of Australia's Security and Forensic needs and takes a more global strategic approach to the important issues. The proposed Centre aims to identify and facilitate research programs that will enhance the security and safety of Australians through improved awareness, integration and coordination of existing resources and strategies for the development of policies, capabilities and procedures for Australia’s protection and fight against crime.

This Centre will bring together a unique combination of professional researchers and end-users in the forensic disciplines; information sciences, biometrics and secure communications; medical, health and environmental sciences; law enforcement, customs and border protection; e-security, decoding and fraud; public sector governance and management; law and evidence; and social and economic modelling.  The Centre will establish essential and unique links between these groups to enable research, policy, and operational synergies that would not otherwise occur.  Together, they will advance national and international security research and develop innovative options for better anticipation and management of potentially serious threats to the health, wellbeing and security of Australians. Additional benefits of the Centre’s research are flow-on consequences for cross-portfolio cooperation and with outcomes that have the potential to advise and guide government and industry.

Security research today focuses on maintaining the viability and protecting various systems from threat and depends upon the contribution of many disciplines working together to ensure that a system is in place that considers all relevant factors.  Protection occurs at a number of strategic levels and includes understanding the nature of a threat, preparation for a possible threat, prevention of the consequences of a threat, response to the consequences of a threat, and recovery from the consequences of a threat. In addressing these strategic levels, it is essential to integrate research that addresses the forensic and legal issues to enable effective  response and recovery. Security threats may be unintentional, accidental and unorganised, or they may be deliberate, organised and malicious (including  criminal and terrorist activities).  While the latter threats occupy much current media discussion, the former class of security threats can have significant devastating social, environmental and economic consequences. As an example, it is now estimated that identity theft is costing Australia in excess of $1.2 billion per annum through credit card, bank account, social security and other such fraudulent theft.

At the University of Canberra we have academics, research staff and students with the expertise, discipline base, resources, collaborative links and research profile to achieve the goals of this Area of Research Strength. This Centre is unique in that it is an integrated "across university" multi-disciplinary Area of Research Strength focusing on the National Research Priority of Safeguarding Australia. Our national capital location also makes us a "natural" to play a networking and linkages role bringing together researchers, professions, industry and government for developing programs and achieving outcomes that respond to national priorities. In addition to our specialised resources, UC also possesses certain infrastructure (eg. electronic courtroom, ISO standard laboratories, etc) that provides us with a unique competitive advantage compared to other universities

The Diploma in Languages is available to all enrolled undergraduate students of the University. This Diploma allows students to complete a three-year sequence in a foreign language while maintaining the requirements for their chosen degree. It is available to beginner students and to students with previous knowledge of the language at a Continuing and Intermediate-Advanced levels (see separate entries). Students enrol in their chosen language at each year level and are normally expected to enrol in less than a full-time load in their degree units for that year. This process will extend the period required to complete both programs to a minimum of four years. Students can start the Diploma units in first or second year of their undergraduate degree. Students starting the Diploma in their third year of undergraduate study can graduate from their degree and finished the remaining Diploma units on a part time basis. 

This Diploma allows students to develop language skills and cultural competence in Chinese, Japanese or Spanish, adding value to and internationalising their degree. Students will acquire language skills that can be used within their professional careers furthering their employment opportunities in Australia or overseas. By the end of the course at the Beginning Level, students will have acquired an intermediate level of communicative skills and cultural competence in their chosen language

A full-time student would normally study four units in each of three semesters, but may extend the course to two years by studying three units per semester. A part-time student would normally study two units per semester. In the first semester, students must take Professional Practice G. The remaining three units may be chosen from a variety of units at graduate (G) or postgraduate (PG) level in the areas of programming, systems analysis and design, computer networks and project and quality management. G level units would normally be selected to satisfy any prerequisites for PG level units to be taken in semesters 2 and 3.

In the second and third semesters, students take PG level units in areas that interest them. The PG level units currently on offer are listed below, grouped by specialisation. Units with an asterisk (*) have a prerequisite of 2 years’ work experience. Please note that in any one year, only a selection of these units will run.
In addition to the list below, individual project units, and (for students specialising in Business Informatics) an internship are available

News acronyms are acronyms of frases used in news writing. They are mostly composed of the first letters of the words in the frase and are best to be written in upper case. The frases are generally in english although some may be in native languages. 

Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. Training patterns are the goals of the training process and not to be confused with the training set.[7] A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of "simple", in accordance with Occam's Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances.[8] A combination of the two that has recently been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words,and the data to be labeled is the training data.

Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. Note also that in some fields, the terminology is different: For example, in community ecology, the term "classification" is used to refer to what is commonly known as "clustering".

The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors.) Typically, features are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of "male" or "female", or a blood type of "A", "B", "AB" or "O"), ordinal (consisting of one of a set of ordered items, e.g., "large", "medium" or "small"), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10).

Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a "best" label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the N-best labels with associated probabilities, for some value of N, instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms:

They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in probability theory. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.)
Correspondingly, they can abstain when the confidence of choosing any particular output is too low.
Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation.

Feature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given.[9] The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of {\displaystyle n} n features the powerset consisting of all {\displaystyle 2^{n}-1} 2^{n}-1 subsets of features need to be explored. The Branch-and-Bound algorithm[10] does reduce this complexity but is intractable for medium to large values of the number of available features {\displaystyle n} n. For a large-scale comparison of feature-selection algorithms see .[11]

Techniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. For example, feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.

Natural language processing[110] (NLP) gives machines the ability to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering[111] and machine translation.[112] Many current approaches use word co-occurrence frequencies to construct syntactic representations of text. "Keyword spotting" strategies for search are popular and scalable but dumb; a search query for "dog" might only match documents with the literal word "dog" and miss a document with the word "poodle". "Lexical affinity" strategies use the occurrence of words such as "accident" to assess the sentiment of a document. Modern statistical NLP approaches can combine all these strategies as well as others, and often achieve acceptable accuracy at the page or paragraph level, but continue to lack the semantic understanding required to classify isolated sentences well. Besides the usual difficulties with encoding semantic commonsense knowledge, existing semantic NLP sometimes scales too poorly to be viable in business applications. Beyond semantic NLP, the ultimate goal of "narrative" NLP is to embody a full understanding of commonsense reasoning.[113]

Machine perception[114] is the ability to use input from sensors (such as cameras (visible spectrum or infrared), microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,[115] facial recognition, and object recognition.[116] Computer vision is the ability to analyze visual input. Such input is usually ambiguous; a giant, fifty-meter-tall pedestrian far away may produce exactly the same pixels as a nearby normal-sized pedestrian, requiring the AI to judge the relative likelihood and reasonableness of different interpretations, for example by using its "object model" to assess that fifty-meter pedestrians do not exist.[117]

Machine learning, a fundamental concept of AI research since the field's inception,[104] is the study of computer algorithms that improve automatically through experience.[105][106]

Unsupervised learning is the ability to find patterns in a stream of input, without requiring a human to label the inputs first.[107] Supervised learning includes both classification and numerical regression, which requires a human to label the input data first. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change.[106] Both classifiers and regression learners can be viewed as "function approximators" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, "spam" or "not spam". Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[108] In reinforcement learning[109] the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space.
