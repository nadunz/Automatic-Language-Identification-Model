Hay varios enfoques estadísticos para la identificación del idioma utilizando diferentes técnicas para clasificar los datos. Una técnica es comparar la compresibilidad del texto con la compresibilidad de los textos en un conjunto de idiomas conocidos. Este enfoque se conoce como medida de distancia basada en información mutua. La misma técnica también se puede usar para construir empíricamente árboles familiares de idiomas que se correspondan estrechamente con los árboles construidos utilizando métodos históricos. [Cita requerida] La medición de la distancia basada en la información mutua es esencialmente equivalente a los métodos basados ​​en modelos más convencionales y generalmente no se considera Sea novedoso o mejor que las técnicas más simples.

Otra técnica, según lo descrito por Cavnar y Trenkle (1994) y Dunning (1994) es crear un modelo de n-gramas lingüísticas a partir de un "texto de entrenamiento" para cada una de las lenguas. Estos modelos pueden basarse en caracteres (Cavnar y Trenkle) o en bytes codificados (Dunning); en este último, se integran la identificación del lenguaje y la detección de la codificación de caracteres. Luego, para cualquier fragmento de texto que necesite ser identificado, se crea un modelo similar, y ese modelo se compara con cada modelo de idioma almacenado. El idioma más probable es aquel con el modelo que es más similar al modelo del texto que necesita ser identificado. Este enfoque puede ser problemático cuando el texto de entrada está en un idioma para el que no hay modelo. En ese caso, el método puede devolver otro lenguaje "más similar" como resultado. También son problemáticos para cualquier enfoque los fragmentos de texto de entrada que están compuestos de varios idiomas, como es común en la Web.

Para un método más reciente, vea  y (2009). Este método puede detectar múltiples idiomas en un texto no estructurado y funciona de manera robusta en textos breves de solo unas pocas palabras: algo con lo que se enfrenta el enfoque de n-gramas.

Un método estadístico más antiguo de Grefenstette se basó en la prevalencia de ciertas palabras funcionales (por ejemplo, "el" en inglés).

Uno de los grandes cuellos de botella de los sistemas de identificación de idiomas es distinguir entre lenguajes estrechamente relacionados. Lenguajes similares como el serbio y el croata o el indonesio y el malayo presentan una importante superposición léxica y estructural, lo que dificulta que los sistemas discriminen entre ellos.

Recientemente, la tarea compartida DSL [1] se organizó proporcionando un conjunto de datos (Tan et al., 2014) que contiene 13 idiomas diferentes (y variedades de idiomas) en seis grupos de idiomas: Grupo A (bosnio, croata, serbio), Grupo B ( Indonesia, Malasia), Grupo C (checo, eslovaco), Grupo D (portugués brasileño, portugués europeo), Grupo E (España peninsular, español argentino), Grupo F (inglés estadounidense, inglés británico). El mejor sistema alcanzó un rendimiento de más del 95% de los resultados (Goutte et al., 2014). Los resultados de la tarea compartida de DSL se describen en Zampieri et al. 2014.

La historia del procesamiento del lenguaje natural generalmente comenzó en la década de 1950, aunque se puede encontrar trabajo de períodos anteriores. En 1950, Alan Turing publicó un artículo titulado "Inteligencia" que proponía lo que ahora se llama la prueba de Turing como criterio de inteligencia.

El experimento de Georgetown en 1954 implicó la traducción automática de más de sesenta oraciones rusas al inglés. Los autores afirmaron que dentro de tres o cinco años, la traducción automática sería un problema resuelto. [2] Sin embargo, el progreso real fue mucho más lento, y después del informe ALPAC en 1966, que encontró que una investigación de diez años no había cumplido con las expectativas, la financiación para la traducción automática se redujo drásticamente. Poco más investigación en traducción automática se llevó a cabo hasta finales de la década de 1980, cuando se desarrollaron los primeros sistemas estadísticos de traducción automática.

Algunos sistemas de procesamiento de lenguaje natural notablemente exitosos desarrollados en la década de 1960 fueron SHRDLU, un sistema de lenguaje natural que trabaja en "mundos de bloques" restringidos con vocabularios restringidos, y ELIZA, una simulación de un psicoterapeuta Rogeriano, escrita por Joseph Weizenbaum entre 1964 y 1966. Sin información sobre el pensamiento o la emoción humana, ELIZA a veces proporcionaba una interacción sorprendentemente similar a la humana. Cuando el "paciente" superó la base de conocimientos muy pequeña, ELIZA podría proporcionar una respuesta genérica, por ejemplo, respondiendo a "Me duele la cabeza" con "¿Por qué dices que te duele la cabeza?".

Durante la década de 1970, muchos programadores comenzaron a escribir "ontologías conceptuales", que estructuraban la información del mundo real en datos comprensibles por computadora. Ejemplos son MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979) y Plot Units (Lehnert 1981 ). Durante este tiempo, se escribieron muchos chatterbots incluyendo PARRY, Racter y Jabberwacky.

Hasta la década de 1980, la mayoría de los sistemas de procesamiento de lenguaje natural se basaban en conjuntos complejos de reglas escritas a mano. Sin embargo, a partir de finales de los años 80, hubo una revolución en el procesamiento del lenguaje natural con la introducción de algoritmos de aprendizaje automático para el procesamiento del lenguaje. Esto se debió tanto al aumento constante en el poder computacional (ver la ley de Moore) como a la disminución gradual del predominio de las teorías chomskyanas de la lingüística (por ejemplo, la gramática transformacional), cuyos fundamentos teóricos desalentaron el tipo de lingüística de corpus que subyace en el enfoque de aprendizaje automático. al procesamiento del lenguaje. [3] Algunos de los algoritmos de aprendizaje automático más antiguos, como los árboles de decisión, produjeron sistemas de reglas rígidas similares a las reglas escritas a mano existentes. Sin embargo, el etiquetado de parte del discurso introdujo el uso de modelos ocultos de Markov en el procesamiento del lenguaje natural, y cada vez más, la investigación se ha centrado en los modelos estadísticos, que toman decisiones suaves y probabilísticas basadas en adjuntar pesos de valor real a las características que componen la entrada. datos. Los modelos de lenguaje de caché en los que se basan muchos sistemas de reconocimiento de voz ahora son ejemplos de dichos modelos estadísticos. Dichos modelos son generalmente más robustos cuando se les da una entrada desconocida, especialmente una entrada que contiene errores (como es muy común para los datos del mundo real), y producen resultados más confiables cuando se integran en un sistema más grande que comprende múltiples subtareas.

Muchos de los éxitos iniciales notables se produjeron en el campo de la traducción automática, debido especialmente al trabajo en IBM Research, donde se desarrollaron modelos estadísticos cada vez más complicados. Estos sistemas pudieron aprovechar los cuerpos textuales multilingües existentes que habían sido producidos por el Parlamento de Canadá y la Unión Europea como resultado de leyes que exigían la traducción de todos los procedimientos gubernamentales a todos los idiomas oficiales de los sistemas de gobierno correspondientes. Sin embargo, la mayoría de los otros sistemas dependían de corpus específicamente desarrollados para las tareas implementadas por estos sistemas, lo cual era (y con frecuencia continúa siendo) una limitación importante en el éxito de estos sistemas. Como resultado, se ha investigado mucho sobre los métodos para aprender de manera más efectiva a partir de cantidades limitadas de datos.

Las investigaciones recientes se han centrado cada vez más en los algoritmos de aprendizaje no supervisados ​​y semi-supervisados. Dichos algoritmos pueden aprender de datos que no se han anotado a mano con las respuestas deseadas, o usando una combinación de datos anotados y no anotados. En general, esta tarea es mucho más difícil que el aprendizaje supervisado, y generalmente produce resultados menos precisos para una cantidad dada de datos de entrada. Sin embargo, hay una enorme cantidad de datos no anotados disponibles (incluido, entre otras cosas, todo el contenido de la World Wide Web), que a menudo puede compensar los resultados inferiores si el algoritmo utilizado tiene una complejidad de tiempo suficientemente baja para Sé práctico.

En la década de 2010, el aprendizaje por representación y los métodos de aprendizaje automático de estilo de red neuronal se generalizaron en el procesamiento del lenguaje natural, debido en parte a una serie de resultados que muestran que tales técnicas [4] [5] pueden lograr resultados de vanguardia. en muchas tareas de lenguaje natural, por ejemplo en modelado de lenguaje, [6] análisis, [7] [8] y muchos otros. Las técnicas populares incluyen el uso de incrustaciones de palabras para capturar las propiedades semánticas de las palabras, y un aumento en el aprendizaje de extremo a extremo de una tarea de nivel superior (por ejemplo, respuesta a preguntas) en lugar de confiar en una serie de tareas intermedias separadas (por ejemplo, etiquetado de parte del discurso y análisis de dependencia). En algunas áreas, este cambio ha conllevado cambios sustanciales en la forma en que se diseñan los sistemas de PNL, de modo que los enfoques basados ​​en redes neuronales profundas pueden verse como un nuevo paradigma distinto del procesamiento estadístico del lenguaje natural. Por ejemplo, el término traducción automática neuronal (NMT) enfatiza el hecho de que los enfoques basados ​​en el aprendizaje profundo para la traducción automática aprenden directamente las transformaciones secuencia a secuencia, obviando la necesidad de pasos intermedios como la alineación de palabras y el modelado del lenguaje que se usaron en estadística. traducción automática (SMT).

En los primeros días, muchos sistemas de procesamiento de idiomas se diseñaron codificando a mano un conjunto de reglas, [9] [10], p. Ej. escribiendo gramáticas o ideando reglas heurísticas para la derivación. Sin embargo, esto rara vez es robusto a la variación del lenguaje natural.

Desde la llamada "revolución estadística" [11] [12] a finales de la década de 1980 y mediados de la década de 1990, gran parte de la investigación sobre el procesamiento del lenguaje natural se ha basado en gran medida en el aprendizaje automático.

El paradigma del aprendizaje automático requiere, en cambio, el uso de la inferencia estadística para aprender automáticamente tales reglas a través del análisis de grandes corpora de ejemplos típicos del mundo real (un corpus (plural, "corpora") es un conjunto de documentos, posiblemente con anotaciones humanas o informáticas. ).

Muchas clases diferentes de algoritmos de aprendizaje automático se han aplicado a tareas de procesamiento de lenguaje natural. Estos algoritmos toman como entrada un gran conjunto de "características" que se generan a partir de los datos de entrada. Algunos de los algoritmos utilizados más tempranamente, como los árboles de decisión, produjeron sistemas de reglas rígidas, similares a los sistemas de reglas escritas a mano que en ese entonces eran comunes. Sin embargo, cada vez más, la investigación se ha centrado en modelos estadísticos, que toman decisiones suaves y probabilísticas basadas en la asignación de ponderaciones de valor real a cada función de entrada. Tales modelos tienen la ventaja de que pueden expresar la certeza relativa de muchas respuestas posibles diferentes en lugar de solo una, produciendo resultados más confiables cuando dicho modelo se incluye como un componente de un sistema más grande.

Los sistemas basados ​​en algoritmos de aprendizaje automático tienen muchas ventajas sobre las reglas producidas a mano:

Los procedimientos de aprendizaje utilizados durante el aprendizaje automático se centran automáticamente en los casos más comunes, mientras que cuando se escriben reglas a mano, a menudo no es obvio hacia dónde debe dirigirse el esfuerzo.

Los procedimientos de aprendizaje automático pueden hacer uso de algoritmos de inferencia estadística para producir modelos que sean robustos a entradas desconocidas (por ejemplo, que contengan palabras o estructuras que no se hayan visto antes) y a entradas erróneas (por ejemplo, con palabras incorrectamente escritas o palabras omitidas accidentalmente). En general, manejar ese aporte con las reglas escritas a mano o, más generalmente, crear sistemas de reglas escritas a mano que tomen decisiones flexibles, es extremadamente difícil, propenso a errores y requiere mucho tiempo.

Los sistemas basados ​​en el aprendizaje automático de las reglas se pueden hacer más precisos simplemente proporcionando más datos de entrada. Sin embargo, los sistemas basados ​​en reglas escritas a mano solo pueden hacerse más precisos al aumentar la complejidad de las reglas, lo cual es una tarea mucho más difícil. En particular, hay un límite a la complejidad de los sistemas basados ​​en reglas hechas a mano, más allá de la cual los sistemas se vuelven cada vez más inmanejables. Sin embargo, crear más datos para ingresar a los sistemas de aprendizaje automático simplemente requiere un aumento correspondiente en el número de horas-hombre trabajadas, generalmente sin aumentos significativos en la complejidad del proceso de anotación.

La detección de codificación de caracteres, la detección de juego de caracteres o la detección de páginas de códigos es el proceso de adivinación heurística de la codificación de caracteres de una serie de bytes que representan texto. Se reconoce que la técnica no es confiable y solo se usa cuando hay metadatos específicos, como un encabezado HTTP Content-Type: o bien no está disponible o se supone que no es confiable.

Este algoritmo generalmente involucra el análisis estadístico de los patrones de bytes, como la distribución de frecuencias de los trigrafos de varios idiomas codificados en cada página de códigos que serán detectados; dicho análisis estadístico también se puede utilizar para realizar la detección del lenguaje. Este proceso no es infalible porque depende de datos estadísticos.

En general, la detección incorrecta del juego de caracteres lleva a mojibake.

Uno de los pocos casos en los que la detección de juegos de caracteres funciona de manera confiable es la detección de UTF-8. Esto se debe al gran porcentaje de secuencias de bytes no válidas en UTF-8, por lo que es muy poco probable que el texto en cualquier otra codificación que use bytes con el conjunto de bits alto pase una prueba de validez de UTF-8. Sin embargo, las rutinas de detección de juego de caracteres mal escritas no ejecutan primero la prueba confiable de UTF-8, y pueden decidir que UTF-8 es otra codificación. Por ejemplo, era común que los sitios web en UTF-8 que contenían el nombre de la ciudad alemana de München se mostraran como München.

UTF-16 es bastante confiable de detectar debido a la gran cantidad de líneas nuevas (U + 000A) y espacios (U + 0020) que se deben encontrar al dividir los datos en palabras de 16 bits, y al hecho de que pocas codificaciones usan 16- palabras pequeñas Este proceso no es infalible; por ejemplo, algunas versiones del sistema operativo Windows detectarían erróneamente la frase "Bush ocultó los hechos" (sin una nueva línea) en ASCII como UTF-16LE en chino.

La detección de conjuntos de caracteres es particularmente poco confiable en Europa, en un entorno de codificaciones ISO-8859 mixtas. Estas son codificaciones de ocho bits estrechamente relacionadas que comparten una superposición en su mitad inferior con ASCII. No hay una forma técnica de diferenciar estas codificaciones y reconocerlas se basa en la identificación de las características del lenguaje, como las frecuencias de letras o la ortografía.

Debido a la falta de fiabilidad de la detección heurística, es mejor etiquetar correctamente los conjuntos de datos con la codificación correcta. Los documentos HTML servidos en la web por HTTP deben tener su codificación establecida fuera de banda mediante el encabezado Content-Type :.

Esta área brindará un enfoque para la investigación e innovación en áreas relacionadas con los sistemas de seguridad e investigación forense y se basará en el prestigio nacional e internacional de la experiencia existente en estos campos en la Universidad de Canberra. Reunirá a investigadores clave de las tres Divisiones de la Universidad, se vinculará con las áreas existentes de investigación de las diferentes disciplinas de la Universidad de Canberra y se asociará con organizaciones externas relacionadas con este campo. Este centro abordará los problemas clave y ampliará los resultados potenciales dentro de este importante campo al reunir a un grupo multidisciplinario de investigadores y profesionales que trabajarán en estrecha colaboración con el gobierno, la industria y las profesiones para lograr los resultados declarados.

Ser un Centro de Excelencia reconocido a nivel nacional e internacional en la investigación que sea proactivo en el suministro de soluciones multidisciplinarias integrales e integradas para los problemas que enfrentan los campos de Seguridad y Forense y que atraiga colaboraciones innovadoras con la industria y los profesionales que tengan resultados que beneficien directamente a Las comunidades nacionales e internacionales.

Para desarrollar un enfoque integral de la investigación centrada en mejorar la seguridad en una variedad de áreas que tendrán resultados novedosos, integrados y sinérgicos en las áreas de ciencias de la información, criptología, sistemas electrónicos y delitos electrónicos, protección de fronteras y costumbres, y sistemas biológicos, Incluyendo el medio ambiente, la salud animal y humana.
Identificar y coordinar programas de investigación innovadores tanto en el campo de la seguridad como en el forense, que involucren al sector de la industria y brinden colaboraciones de investigación sinérgicas vinculadas con la industria y programas para estudiantes de posgrado.
Para ser proactivos en la identificación de áreas que requieren avances estratégicos de conocimiento y soluciones a las amenazas a la seguridad y evaluaciones forenses nuevas y existentes, y difundir los resultados para la implementación dentro de los sectores relevantes de la industria pública o privada; y
Proporcionar beneficios reales y futuros en los campos de seguridad y forenses a través de la participación activa de los especialistas legales y de políticas que resultarán en una reducción de la delincuencia a través de un mejor análisis de evidencia para los sistemas de condena y prevención.

Estos objetivos se medirán por los resultados y productos que resulten de la investigación y la utilización de estos dentro de la industria y el sector profesional, la influencia que tienen en la sociedad, en la política que afecta y mediante el beneficio medible que proporcionan a la policía y al procedimiento legal.

La última década ha sido testigo de importantes cambios tanto en nuestros requisitos de seguridad nacional como en los patrones de conflicto humano. En consecuencia, los gobiernos exigen soluciones integradas, coordinadas y rentables para responder a un espectro de amenazas más amplio. Existe una clara necesidad de una seguridad similar y facilitación de la investigación forense en Australia. Este Centro es único porque está diseñado para tratar un análisis interdisciplinario más amplio de las necesidades de Seguridad y Forenses de Australia y adopta un enfoque estratégico más global para los temas importantes. El Centro propuesto tiene como objetivo identificar y facilitar los programas de investigación que mejorarán la seguridad y protección de los australianos a través de un mejor conocimiento, integración y coordinación de los recursos y estrategias existentes para el desarrollo de políticas, capacidades y procedimientos para la protección y lucha de Australia contra la delincuencia.

Este Centro reunirá una combinación única de investigadores profesionales y usuarios finales en las disciplinas forenses; ciencias de la información, biometría y comunicaciones seguras; ciencias médicas, sanitarias y ambientales; aplicación de la ley, aduanas y protección fronteriza; e-seguridad, decodificación y fraude; gobernanza y gestión del sector público; ley y evidencia; y modelización social y económica. El Centro establecerá vínculos esenciales y únicos entre estos grupos para permitir la investigación, la política y las sinergias operativas que de otra manera no ocurrirían. Juntos, avanzarán en la investigación de seguridad nacional e internacional y desarrollarán opciones innovadoras para una mejor anticipación y manejo de amenazas potencialmente graves para la salud, el bienestar y la seguridad de los australianos. Los beneficios adicionales de la investigación del Centro son las consecuencias para la cooperación entre las carteras y los resultados que tienen el potencial de asesorar y orientar al gobierno y la industria.

La investigación de seguridad hoy se enfoca en mantener la viabilidad y proteger varios sistemas contra amenazas y depende de la contribución de muchas disciplinas que trabajan juntas para garantizar que exista un sistema que tenga en cuenta todos los factores relevantes. La protección se produce en varios niveles estratégicos e incluye comprender la naturaleza de una amenaza, la preparación para una posible amenaza, la prevención de las consecuencias de una amenaza, la respuesta a las consecuencias de una amenaza y la recuperación de las consecuencias de una amenaza. Al abordar estos niveles estratégicos, es esencial integrar la investigación que aborde los problemas legales y forenses para permitir una respuesta y recuperación efectivas. Las amenazas a la seguridad pueden ser involuntarias, accidentales y no organizadas, o pueden ser deliberadas, organizadas y maliciosas (incluidas las actividades delictivas y terroristas). Si bien las últimas amenazas ocupan mucho debate actual en los medios, la clase anterior de amenazas a la seguridad puede tener importantes consecuencias sociales, ambientales y económicas devastadoras. Como ejemplo, ahora se estima que el robo de identidad le está costando a Australia más de $ 1,2 mil millones por año a través de tarjetas de crédito, cuentas bancarias, seguridad social y otros robos fraudulentos.

En la Universidad de Canberra tenemos académicos, personal de investigación y estudiantes con experiencia, base de disciplina, recursos, enlaces de colaboración y perfil de investigación para alcanzar los objetivos de esta Área de Fuerza de Investigación. Este Centro es único en el sentido de que es un Área multidisciplinaria integrada de "Fuerza de Investigación" que se centra en la Prioridad de Investigación Nacional de Salvaguardar Australia. Nuestra ubicación en la capital nacional también nos convierte en "naturales" para desempeñar un papel de redes y vínculos que reúne a investigadores, profesiones, industria y gobierno para desarrollar programas y lograr resultados que respondan a las prioridades nacionales. Además de nuestros recursos especializados, UC también posee cierta infraestructura (por ejemplo, sala de audiencias electrónicas, laboratorios de la norma ISO, etc.) que nos brinda una ventaja competitiva única en comparación con otras universidades.

El Diploma en Idiomas está disponible para todos los estudiantes de pregrado matriculados de la Universidad. Este Diploma les permite a los estudiantes completar una secuencia de tres años en un idioma extranjero mientras se mantienen los requisitos para el grado elegido. Está disponible para estudiantes principiantes y para estudiantes con conocimientos previos del idioma en los niveles Continuo e Intermedio-Avanzado (ver entradas separadas). Los estudiantes se inscriben en el idioma elegido en cada nivel de año y normalmente se espera que se inscriban en una carga de menos de tiempo completo en sus unidades de grado para ese año. Este proceso extenderá el período requerido para completar ambos programas a un mínimo de cuatro años. Los estudiantes pueden comenzar las unidades de Diploma en el primer o segundo año de su título universitario. Los estudiantes que comienzan el Diploma en su tercer año de estudios universitarios pueden graduarse de su título y terminar las unidades restantes del Diploma a tiempo parcial.

Este Diploma permite a los estudiantes desarrollar habilidades lingüísticas y competencia cultural en chino, japonés o español, agregando valor e internacionalizando su título. Los estudiantes adquirirán habilidades lingüísticas que se pueden utilizar en sus carreras profesionales para mejorar sus oportunidades de empleo en Australia o en el extranjero. Al finalizar el curso en el nivel inicial, los estudiantes habrán adquirido un nivel intermedio de habilidades comunicativas y competencia cultural en el idioma que elijan.

Un estudiante a tiempo completo normalmente estudiaría cuatro unidades en cada uno de tres semestres, pero puede extender el curso a dos años estudiando tres unidades por semestre. Un estudiante a tiempo parcial normalmente estudiaría dos unidades por semestre. En el primer semestre, los estudiantes deben realizar la Práctica profesional G. Las tres unidades restantes pueden seleccionarse de una variedad de unidades a nivel de posgrado (G) o posgrado (PG) en las áreas de programación, análisis y diseño de sistemas, redes de computadoras y proyectos. y gestión de la calidad. Las unidades de nivel G normalmente se seleccionarán para satisfacer cualquier requisito previo para las unidades de nivel PG que se tomarán en los semestres 2 y 3.

En el segundo y tercer semestre, los estudiantes toman unidades de nivel PG en áreas que les interesan. Las unidades de nivel PG actualmente en oferta se enumeran a continuación, agrupadas por especialización. Las unidades con un asterisco (*) tienen un requisito previo de 2 años de experiencia laboral. Tenga en cuenta que en cualquier año, solo se ejecutará una selección de estas unidades.
Además de la lista a continuación, las unidades de proyectos individuales y (para los estudiantes que se especializan en informática empresarial) están disponibles una pasantía

Los acrónimos de noticias son acrónimos de frases utilizadas en la redacción de noticias. En su mayoría están compuestas por las primeras letras de las palabras en la frase y es mejor escribirlas en mayúsculas. Las frases están generalmente en inglés, aunque algunas pueden estar en idiomas nativos.

El reconocimiento de patrones generalmente se clasifica según el tipo de procedimiento de aprendizaje utilizado para generar el valor de salida. El aprendizaje supervisado asume que se ha proporcionado un conjunto de datos de entrenamiento (el conjunto de entrenamiento), que consiste en un conjunto de instancias que se han etiquetado correctamente a mano con el resultado correcto. Los patrones de entrenamiento son los objetivos del proceso de entrenamiento y no deben confundirse con el conjunto de entrenamiento. [7] Luego, un procedimiento de aprendizaje genera un modelo que intenta cumplir dos objetivos a veces conflictivos: tener el mejor rendimiento posible en los datos de entrenamiento y generalizar lo mejor posible a nuevos datos (por lo general, esto significa ser lo más simple posible, para una definición técnica) de "simple", de acuerdo con Occam's Razor, discutido a continuación). El aprendizaje no supervisado, por otro lado, supone datos de entrenamiento que no se han etiquetado a mano, e intenta encontrar patrones inherentes en los datos que luego se pueden usar para determinar el valor de salida correcto para las nuevas instancias de datos. [8] Una combinación de los dos que se han explorado recientemente es el aprendizaje semi-supervisado, que utiliza una combinación de datos etiquetados y no etiquetados (por lo general, un pequeño conjunto de datos etiquetados combinados con una gran cantidad de datos no etiquetados). Tenga en cuenta que en los casos de aprendizaje no supervisado, es posible que no haya datos de capacitación en absoluto para hablar; en otras palabras, y los datos a etiquetar son los datos de entrenamiento.

Tenga en cuenta que a veces se utilizan términos diferentes para describir los correspondientes procedimientos de aprendizaje supervisados ​​y no supervisados ​​para el mismo tipo de resultado. Por ejemplo, el equivalente no supervisado de la clasificación se conoce normalmente como agrupamiento, basado en la percepción común de la tarea que implica que no hay datos de entrenamiento, y agrupar los datos de entrada en agrupamientos según una medida de similitud inherente (por ejemplo, la distancia entre instancias, consideradas como vectores en un espacio vectorial multidimensional), en lugar de asignar cada instancia de entrada a una de un conjunto de clases predefinidas. Tenga en cuenta también que en algunos campos, la terminología es diferente: por ejemplo, en ecología de la comunidad, el término "clasificación" se usa para referirse a lo que comúnmente se conoce como "agrupamiento".

La pieza de datos de entrada para la cual se genera un valor de salida se denomina formalmente una instancia. La instancia se describe formalmente por un vector de características, que juntas constituyen una descripción de todas las características conocidas de la instancia. (Estos vectores de características se pueden ver como puntos de definición en un espacio multidimensional apropiado, y los métodos para manipular vectores en espacios de vectores se pueden aplicar a ellos de manera correspondiente, como el cálculo del producto de puntos o el ángulo entre dos vectores). Por lo general, las características son: categórico (también conocido como nominal, es decir, que consiste en uno de un conjunto de elementos no ordenados, como un género de "masculino" o "femenino", o un tipo de sangre de "A", "B", "AB" o " O "), ordinal (que consiste en uno de un conjunto de elementos ordenados, por ejemplo," grande "," mediano "o" pequeño "), con valores enteros (por ejemplo, un recuento del número de ocurrencias de una palabra en particular en un correo electrónico) o de valor real (por ejemplo, una medición de la presión arterial). A menudo, los datos categóricos y ordinales se agrupan juntos; Lo mismo ocurre con los datos de valores enteros y valores reales. Además, muchos algoritmos funcionan solo en términos de datos categóricos y requieren que los datos de valores reales o valores enteros se discreticen en grupos (por ejemplo, menos de 5, entre 5 y 10, o más de 10).

Muchos algoritmos comunes de reconocimiento de patrones son de naturaleza probabilística, ya que usan la inferencia estadística para encontrar la mejor etiqueta para una instancia determinada. A diferencia de otros algoritmos, que simplemente generan una "mejor" etiqueta, a menudo los algoritmos probabilísticos también generan una probabilidad de que la instancia sea descrita por la etiqueta dada. Además, muchos algoritmos probabilísticos generan una lista de las N-mejores etiquetas con probabilidades asociadas, para algún valor de N, en lugar de simplemente una única etiqueta. Cuando el número de etiquetas posibles es bastante pequeño (por ejemplo, en el caso de la clasificación), se puede establecer N para que se obtenga la probabilidad de todas las etiquetas posibles. Los algoritmos probabilísticos tienen muchas ventajas sobre los algoritmos no probabilísticos:

Producen un valor de confianza asociado con su elección. (Tenga en cuenta que algunos otros algoritmos también pueden generar valores de confianza, pero en general, solo para algoritmos probabilísticos es este valor matemáticamente fundamentado en la teoría de la probabilidad. A los valores de confianza no probabilísticos no se les puede dar ningún significado específico, y solo se pueden usar para compararlos con Otros valores de confianza emitidos por el mismo algoritmo.
En consecuencia, pueden abstenerse cuando la confianza de elegir cualquier producto en particular es demasiado baja.
Debido a la salida de probabilidades, los algoritmos de reconocimiento de patrones probabilísticos pueden incorporarse más eficazmente en tareas de aprendizaje de máquina más grandes, de forma que se evite parcial o completamente el problema de la propagación de errores.

Los algoritmos de selección de funciones intentan eliminar directamente las funciones redundantes o irrelevantes. Se ha dado una introducción general a la selección de características que resume los enfoques y desafíos. [9] La complejidad de la selección de características es, debido a su carácter no monótono, un problema de optimización en el que dado un total de {\ displaystyle n} n presenta el conjunto de poderes que consta de todos {\ displaystyle 2 ^ {n} -1} 2 ^ { n} -1 subconjuntos de características deben ser explorados. El algoritmo Branch-and-Bound [10] reduce esta complejidad pero es intratable para valores medianos y grandes del número de funciones disponibles {\ displaystyle n} n. Para una comparación a gran escala de algoritmos de selección de características, vea. [11]

Las técnicas para transformar los vectores de características en bruto (extracción de características) a veces se usan antes de la aplicación del algoritmo de coincidencia de patrones. Por ejemplo, los algoritmos de extracción de características intentan reducir un vector de características de gran dimensionalidad a un vector de menor dimensión con el que es más fácil trabajar y codifica menos redundancia, utilizando técnicas matemáticas como el análisis de componentes principales (PCA). La distinción entre selección de características y extracción de características es que las características resultantes después de la extracción de características son de un tipo diferente a las características originales y pueden no ser fácilmente interpretables, mientras que las características que quedan después de la selección de características son simplemente un subconjunto de las características originales .

El procesamiento del lenguaje natural [110] (PNL) brinda a las máquinas la capacidad de leer y comprender el lenguaje humano. Un sistema de procesamiento de lenguaje natural suficientemente poderoso permitiría las interfaces de usuario en lenguaje natural y la adquisición de conocimientos directamente de fuentes escritas por humanos, como textos de noticias. Algunas aplicaciones directas del procesamiento del lenguaje natural incluyen la recuperación de información, la extracción de texto, la respuesta a preguntas [111] y la traducción automática. [112] Muchos enfoques actuales utilizan frecuencias de co-ocurrencia de palabras para construir representaciones sintácticas de texto. Las estrategias de búsqueda de "localización de palabras clave" son populares y escalables pero tontas; una consulta de búsqueda para "perro" solo puede hacer coincidir los documentos con la palabra literal "perro" y perder un documento con la palabra "caniche". Las estrategias de "afinidad léxica" utilizan la aparición de palabras como "accidente" para evaluar el sentimiento de un documento. Los enfoques estadísticos modernos de la PNL pueden combinar todas estas estrategias y otras, y a menudo logran una precisión aceptable en el nivel de la página o del párrafo, pero siguen careciendo de la comprensión semántica necesaria para clasificar bien las oraciones aisladas. 

Además de las dificultades habituales para codificar el conocimiento del sentido común semántico, la PNL semántica existente a veces se escala demasiado mal para ser viable en aplicaciones empresariales. Más allá de la PNL semántica, el objetivo final de la PNL "narrativa" es encarnar una comprensión completa del razonamiento de sentido común. [113]
La percepción de la máquina [114] es la capacidad de utilizar la entrada de sensores (como cámaras (espectro visible o infrarrojo), micrófonos, señales inalámbricas y sensores lidar, sonar, de radar y táctiles activos) para deducir aspectos del mundo. Las aplicaciones incluyen reconocimiento de voz, [115] reconocimiento facial y reconocimiento de objetos. [116] La visión de computadora es la capacidad de analizar la entrada visual. Tal entrada suele ser ambigua; un peatón gigante de cincuenta metros de altura puede producir exactamente los mismos píxeles que un peatón cercano de tamaño normal, lo que requiere que la IA juzgue la probabilidad relativa y la razonabilidad de diferentes interpretaciones, por ejemplo, utilizando su "modelo de objeto" para evaluar que no existen peatones de cincuenta metros [117].

El aprendizaje automático, un concepto fundamental de la investigación en IA desde el inicio del campo, [104] es el estudio de algoritmos informáticos que mejoran automáticamente a través de la experiencia. [105] [106]

El aprendizaje no supervisado es la capacidad de encontrar patrones en un flujo de entrada, sin necesidad de que un humano etiquete las entradas primero. [107] El aprendizaje supervisado incluye tanto la clasificación como la regresión numérica, lo que requiere que un humano etiquete los datos de entrada primero. La clasificación se usa para determinar a qué categoría pertenece algo, después de ver una serie de ejemplos de cosas de varias categorías. La regresión es el intento de producir una función que describa la relación entre entradas y salidas y predice cómo deben cambiarse las salidas a medida que cambian las entradas. [106] Tanto los clasificadores como los estudiantes de regresión se pueden ver como "aproximadores de funciones" que intentan aprender una función desconocida (posiblemente implícita); por ejemplo, un clasificador de spam se puede ver como aprender una función que se asigna del texto de un correo electrónico a una de dos categorías, "spam" o "no spam". La teoría del aprendizaje computacional puede evaluar a los alumnos por la complejidad computacional, por la complejidad de la muestra (cuántos datos se requieren), o por otras nociones de optimización. [108] En el aprendizaje por refuerzo [109], el agente es recompensado por las buenas respuestas y castigado por las malas. El agente usa esta secuencia de recompensas y castigos para formar una estrategia para operar en su espacio problemático.