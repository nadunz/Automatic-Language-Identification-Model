Esistono diversi approcci statistici all'identificazione della lingua utilizzando diverse tecniche per classificare i dati. Una tecnica consiste nel confrontare la compressibilità del testo con la comprimibilità dei testi in un insieme di lingue conosciute. Questo approccio è noto come misura di distanza basata sull'informazione reciproca. La stessa tecnica può anche essere usata per costruire empiricamente alberi genealogici di lingue che corrispondono strettamente agli alberi costruiti usando metodi storici. [Citazione necessaria] La misura della distanza basata sull'informazione reciproca è essenzialmente equivalente a metodi basati su modelli più convenzionali e generalmente non è considerata essere romanzo o meglio delle tecniche più semplici.

Un'altra tecnica, come descritto da Cavnar e Trenkle (1994) e Dunning (1994) consiste nel creare un modello di n-grammo linguistico da un "testo di addestramento" per ciascuna delle lingue. Questi modelli possono essere basati su caratteri (Cavnar e Trenkle) o codificati (Dunning); in quest'ultimo, sono integrati l'identificazione della lingua e il rilevamento della codifica dei caratteri. Quindi, per ogni testo che deve essere identificato, viene creato un modello simile e tale modello viene confrontato con ciascun modello di linguaggio memorizzato. La lingua più probabile è quella con il modello che è più simile al modello del testo che deve essere identificato. Questo approccio può essere problematico quando il testo di input è in una lingua per la quale non esiste un modello. In tal caso, il metodo può restituire un altro linguaggio "più simile" come risultato. Anche problematico per qualsiasi approccio sono pezzi di testo di input che sono composti da più lingue, come è comune sul Web.

Per un metodo più recente, vedi e (2009). Questo metodo è in grado di rilevare più lingue in una parte di testo non strutturata e funziona in modo efficace su brevi testi di poche parole: qualcosa con cui gli approcci dell'n-gram fanno fatica.

Un metodo statistico precedente di Grefenstette era basato sulla prevalenza di alcune parole di funzione (ad es. "Il" in inglese).

Uno dei grandi bottleneck dei sistemi di identificazione della lingua è quello di distinguere tra lingue strettamente correlate. Linguaggi simili come il serbo e il croato o l'indonesiano e il malese presentano significative sovrapposizioni lessicali e strutturali, che rendono difficile per i sistemi discriminare tra loro.

Recentemente, l'attività condivisa DSL [1] è stata organizzata fornendo un set di dati (Tan et al., 2014) contenente 13 lingue diverse (e varietà linguistiche) in sei gruppi linguistici: Gruppo A (bosniaco, croato, serbo), gruppo B ( Indonesiano, malese), gruppo C (ceco, slovacco), gruppo D (portoghese brasiliano, portoghese europeo), gruppo E (Spagna peninsulare, spagnolo argentino), gruppo F (inglese americano, inglese britannico). Il miglior sistema ha raggiunto risultati superiori al 95% (Goutte et al., 2014). I risultati dell'attività condivisa DSL sono descritti in Zampieri et al. Il 2014.

La storia dell'elaborazione del linguaggio naturale è iniziata generalmente negli anni '50, sebbene si possa trovare lavoro in epoche precedenti. Nel 1950, Alan Turing pubblicò un articolo intitolato "Intelligence" che proponeva quello che ora viene chiamato test di Turing come criterio di intelligenza.

L'esperimento di Georgetown nel 1954 ha comportato la traduzione completamente automatica di oltre sessanta frasi russe in inglese. Gli autori hanno affermato che entro tre o cinque anni, la traduzione automatica sarebbe un problema risolto. [2] Tuttavia, i progressi reali sono stati molto più lenti e, dopo il rapporto ALPAC del 1966, che ha rilevato che la ricerca di dieci anni non ha soddisfatto le aspettative, i finanziamenti per la traduzione automatica sono stati drasticamente ridotti. Poco ulteriori ricerche in traduzione automatica sono state condotte fino alla fine degli anni '80, quando furono sviluppati i primi sistemi di traduzione di macchine statistiche.

Alcuni sistemi di elaborazione del linguaggio naturale di notevole successo sviluppati negli anni '60 erano SHRDLU, un sistema di linguaggio naturale che operava in ristretti "mondi di blocchi" con vocabolario limitato ed ELIZA, una simulazione di uno psicoterapeuta di Rogerian, scritta da Joseph Weizenbaum tra il 1964 e il 1966. Uso quasi nessuna informazione sul pensiero o l'emozione umana, a volte l'ELIZA forniva un'interazione sorprendentemente umana. Quando il "paziente" ha superato la piccolissima base di conoscenze, ELIZA potrebbe fornire una risposta generica, ad esempio, rispondendo a "La mia testa fa male" con "Perché dici che fa male la testa?".

Durante gli anni '70, molti programmatori iniziarono a scrivere "ontologie concettuali", che strutturavano le informazioni del mondo reale in dati comprensibili al computer. Esempi sono MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979) e Plot Units (Lehnert 1981 ). Durante questo periodo, sono stati scritti molti chatterbots tra cui PARRY, Racter e Jabberwacky.

Fino agli anni '80, la maggior parte dei sistemi di elaborazione del linguaggio naturale erano basati su complesse serie di regole scritte a mano. A partire dalla fine degli anni '80, tuttavia, c'è stata una rivoluzione nell'elaborazione del linguaggio naturale con l'introduzione di algoritmi di apprendimento automatico per l'elaborazione del linguaggio. Ciò era dovuto sia al costante aumento della potenza computazionale (vedere la legge di Moore) sia alla graduale diminuzione del dominio delle teorie chomskiane della linguistica (ad esempio la grammatica trasformazionale), le cui basi teoriche scoraggiavano la sorta di linguistica dei corpus che sottostà all'approccio machine-learning all'elaborazione del linguaggio. [3] Alcuni dei primi algoritmi di apprendimento automatico utilizzati, come gli alberi decisionali, producevano sistemi di regole if-then rigide simili alle regole scritte a mano esistenti. Tuttavia, la codifica di parte del discorso ha introdotto l'uso di modelli Markov nascosti per l'elaborazione del linguaggio naturale e, in misura crescente, la ricerca si è concentrata su modelli statistici, che prendono decisioni morbide e probabilistiche basate sul collegamento di pesi reali alle caratteristiche che costituiscono l'input dati. I modelli di linguaggio della cache su cui ora si basano molti sistemi di riconoscimento vocale sono esempi di tali modelli statistici. Tali modelli sono generalmente più robusti se vengono forniti input non familiari, in particolare input che contengono errori (come è molto comune per i dati del mondo reale) e producono risultati più affidabili se integrati in un sistema più ampio che comprende più sottoattività.

Molti dei primi successi degni di nota si sono verificati nel campo della traduzione automatica, dovuto in particolare al lavoro presso la IBM Research, dove sono stati sviluppati modelli statistici successivamente più complessi. Questi sistemi sono stati in grado di sfruttare i corpora testuali multilingue che erano stati prodotti dal Parlamento del Canada e dall'Unione Europea a seguito delle leggi che chiedevano la traduzione di tutti i procedimenti governativi in ​​tutte le lingue ufficiali dei corrispondenti sistemi di governo. Tuttavia, la maggior parte degli altri sistemi dipendeva da corpi sviluppati specificamente per i compiti implementati da questi sistemi, che era (e spesso continua ad essere) un limite importante nel successo di questi sistemi. Di conseguenza, sono state condotte molte ricerche su metodi per l'apprendimento più efficace da una quantità limitata di dati.

La ricerca recente si è sempre più concentrata su algoritmi di apprendimento non supervisionati e semi-supervisionati. Tali algoritmi sono in grado di apprendere dai dati che non sono stati annotati a mano con le risposte desiderate o utilizzando una combinazione di dati annotati e non annotati. In genere, questo compito è molto più difficile dell'apprendimento supervisionato e in genere produce risultati meno accurati per una determinata quantità di dati di input. Tuttavia, vi è un'enorme quantità di dati non annotati disponibili (incluso, tra l'altro, l'intero contenuto del World Wide Web), che spesso può compensare i risultati inferiori se l'algoritmo utilizzato ha una complessità temporale sufficientemente bassa per essere pratico

Negli anni 2010, l'apprendimento della rappresentazione e i metodi di apprendimento automatico della rete neurale profonda sono diventati molto diffusi nell'elaborazione del linguaggio naturale, in parte a causa di una serie di risultati che dimostrano che tali tecniche [4] [5] possono ottenere risultati allo stato dell'arte in molti compiti in linguaggio naturale, ad esempio nella modellazione del linguaggio, [6] parsing, [7] [8] e molti altri. Tecniche popolari includono l'uso di embeddings di parole per acquisire proprietà semantiche di parole e un aumento dell'apprendimento end-to-end di un'attività di livello superiore (ad esempio, la risposta alle domande) invece di affidarsi a una pipeline di compiti intermedi separati (ad es. tag di parte del discorso e analisi delle dipendenze). In alcune aree, questo cambiamento ha comportato cambiamenti sostanziali nel modo in cui i sistemi di PNL sono progettati, in modo tale che approcci basati su reti neurali profonde possano essere visti come un nuovo paradigma distinto dall'elaborazione statistica del linguaggio naturale. Ad esempio, il termine Neural Machine Translation (NMT) sottolinea il fatto che approcci basati sulla formazione profonda alla traduzione automatica imparano direttamente trasformazioni da sequenza a sequenza, ovviando alla necessità di passaggi intermedi come l'allineamento delle parole e la modellazione del linguaggio che sono stati utilizzati in statistica traduzione automatica (SMT).

Agli albori, molti sistemi di elaborazione della lingua erano progettati codificando a mano una serie di regole, [9] [10], ad es. scrivendo grammatiche o inventando regole euristiche per lo stemming Tuttavia, questo è raramente robusto alla variazione del linguaggio naturale.

Dalla cosiddetta "rivoluzione statistica" [11] [12] alla fine degli anni '80 e alla metà degli anni '90, molte ricerche di elaborazione del linguaggio naturale hanno fatto molto affidamento sull'apprendimento automatico.

Il paradigma di apprendimento automatico chiama invece l'uso dell'inferenza statistica per apprendere automaticamente tali regole attraverso l'analisi di grandi corpora di esempi tipici del mondo reale (un corpus (plurale, "corpora") è un insieme di documenti, possibilmente con annotazioni umane o informatiche ).

Molte classi differenti di algoritmi di apprendimento automatico sono state applicate alle attività di elaborazione della lingua naturale. Questi algoritmi prendono come input un ampio set di "caratteristiche" che vengono generate dai dati di input. Alcuni dei primi algoritmi usati, come gli alberi decisionali, producevano sistemi di regole if-then dure simili ai sistemi di regole scritte a mano che erano allora comuni. Sempre più spesso, la ricerca si è concentrata su modelli statistici, che prendono decisioni morbide e probabilistiche basate sul collegamento di pesi di valore reale a ciascuna caratteristica di input. Tali modelli hanno il vantaggio di poter esprimere la relativa certezza di molte diverse risposte possibili piuttosto che una sola, producendo risultati più affidabili quando tale modello è incluso come componente di un sistema più grande.

I sistemi basati su algoritmi di apprendimento automatico hanno molti vantaggi rispetto alle regole prodotte a mano:

Le procedure di apprendimento utilizzate durante l'apprendimento automatico si concentrano automaticamente sui casi più comuni, mentre quando si scrivono le regole a mano spesso non è affatto ovvio dove lo sforzo debba essere diretto.

Le procedure di apprendimento automatico possono utilizzare algoritmi di inferenza statistica per produrre modelli che sono robusti a input non familiari (ad esempio contenenti parole o strutture che non sono mai state viste prima) e a input errati (ad esempio parole o parole errate omesse). In generale, gestire tali input con garbo con regole scritte a mano, o, più in generale, creando sistemi di regole scritte a mano che prendono decisioni soft, è estremamente difficile, soggetto a errori e che richiede tempo.

I sistemi basati sull'apprendimento automatico delle regole possono essere resi più accurati semplicemente fornendo più dati di input. Tuttavia, i sistemi basati su regole scritte a mano possono essere resi più accurati solo aumentando la complessità delle regole, che è un compito molto più difficile. In particolare, c'è un limite alla complessità dei sistemi basati su regole fatte a mano, oltre le quali i sistemi diventano sempre più ingestibili. Tuttavia, la creazione di più dati da inserire nei sistemi di apprendimento automatico richiede semplicemente un corrispondente aumento del numero di ore uomo lavorate, in genere senza significativi aumenti nella complessità del processo di annotazione.

Il rilevamento della codifica dei caratteri, il rilevamento del set di caratteri o il rilevamento della code page è il processo di induzione euristica della codifica dei caratteri di una serie di byte che rappresentano il testo. Si riconosce che la tecnica è inaffidabile e viene utilizzata solo quando metadati specifici, come un'intestazione Content-Type HTTP: non sono disponibili o si presume che non siano attendibili.

Questo algoritmo di solito comporta l'analisi statistica dei modelli di byte, come la distribuzione di frequenza dei trigrafi di vari linguaggi codificati in ciascuna tabella codici che verranno rilevati; tale analisi statistica può anche essere utilizzata per eseguire il rilevamento della lingua. Questo processo non è infallibile perché dipende da dati statistici.

In generale, il rilevamento di caratteri non corretti porta a mojibake.

Uno dei pochi casi in cui il rilevamento charset funziona in modo affidabile sta rilevando UTF-8. Ciò è dovuto all'elevata percentuale di sequenze di byte non valide in UTF-8, pertanto è estremamente improbabile che il testo in qualsiasi altra codifica che utilizza byte con il set di bit elevato superi un test di validità UTF-8. Tuttavia, le routine di rilevamento charset scritte male non eseguono prima il test UTF-8 affidabile e possono decidere che UTF-8 sia un'altra codifica. Ad esempio, era comune che i siti Web in UTF-8 contenenti il ​​nome della città tedesca di Monaco venissero mostrati come MÃ¼nchen.

UTF-16 è abbastanza affidabile da rilevare a causa dell'elevato numero di newline (U + 000A) e degli spazi (U + 0020) che dovrebbero essere trovati quando si dividono i dati in parole da 16 bit, e il fatto che poche codifiche usano 16- bit parole. Questo processo non è infallibile; ad esempio, alcune versioni del sistema operativo Windows potrebbero rilevare erroneamente la frase "Bush ha nascosto i fatti" (senza una nuova riga) in ASCII come cinese UTF-16LE.

Il rilevamento del Charset è particolarmente inaffidabile in Europa, in un ambiente con codifiche ISO-8859 miste. Queste sono codifiche a otto bit strettamente correlate che condividono una sovrapposizione nella loro metà inferiore con ASCII. Non esiste un modo tecnico per distinguere queste codifiche e riconoscerle si basa sull'identificazione di caratteristiche linguistiche, come le frequenze delle lettere o le grafie.

A causa della inaffidabilità del rilevamento euristico, è meglio etichettare correttamente i set di dati con la codifica corretta. I documenti HTML pubblicati sul web da HTTP dovrebbero avere la codifica dichiarata fuori banda utilizzando l'intestazione Content-Type :.

Questa area fornirà un focus per la ricerca e l'innovazione in aree relative ai sistemi di sicurezza e indagini forensi e si baserà sulla posizione nazionale e internazionale delle competenze esistenti in questi campi presso l'Università di Canberra. Riunirà ricercatori chiave di tutte e tre le divisioni dell'Università, collegando le aree esistenti di forza di ricerca delle diverse discipline dell'Università di Canberra e collaborando con organizzazioni esterne legate a questo settore. Questo Centro affronterà questioni chiave ed estenderà i potenziali risultati in questo importante settore riunendo un gruppo multidisciplinare di ricercatori e professionisti che lavoreranno a stretto contatto con il governo, l'industria e le professioni per raggiungere i risultati dichiarati

Essere un centro di eccellenza riconosciuto a livello nazionale e internazionale nella ricerca che sia proattivo nella fornitura di soluzioni multidisciplinari complete e integrate ai problemi che affliggono il campo della sicurezza e della medicina legale e che attragga collaborazioni innovative con l'industria e i professionisti che hanno benefici diretti per le comunità nazionali e internazionali

Sviluppare un approccio globale alla ricerca incentrato sul miglioramento della sicurezza in una serie di settori che avranno risultati nuovi, integrati e sinergici nei settori delle scienze dell'informazione, crittografia, sistemi elettronici ed e-crime, protezione delle frontiere e dogane e sistemi biologici, compreso l'ambiente, la salute animale e umana.
Identificare e coordinare programmi di ricerca innovativi nel campo della sicurezza e della medicina legale che coinvolgono il settore industriale e forniscono sinergiche collaborazioni di ricerca legate all'industria e programmi per studenti post-laurea;
Essere proattivi nell'individuare le aree che richiedono un avanzamento strategico delle conoscenze e soluzioni alle minacce alla sicurezza nuove ed esistenti e alle valutazioni forensi e diffondere i risultati per l'attuazione all'interno dei settori industriali pubblici o privati ​​interessati; e
Fornire benefici effettivi e futuri nei settori della sicurezza e della medicina legale attraverso l'impegno attivo degli specialisti legali e politici che si tradurranno in una riduzione della criminalità attraverso una migliore analisi delle prove per i sistemi di convinzione e prevenzione.

Questi obiettivi saranno misurati dai risultati e dai risultati derivanti dalla ricerca e dall'utilizzo di questi all'interno dell'industria e del settore professionale, dall'influenza che hanno sulla società, dall'influenza sulla politica e dai benefici misurabili che forniscono alle forze dell'ordine e alla procedura legale

L'ultimo decennio ha visto cambiamenti importanti sia per le nostre esigenze di sicurezza nazionale che per i modelli di conflitto umano. Di conseguenza, i governi richiedono soluzioni integrate, coordinate ed economicamente vantaggiose per rispondere a un più ampio spettro di minacce. Esiste una chiara necessità di una simile facilitazione della ricerca scientifica e forense in Australia. Questo Centro è unico in quanto è progettato per affrontare una più ampia analisi interdisciplinare delle esigenze di sicurezza e forense dell'Australia e adotta un approccio strategico più globale per le questioni importanti. Il Centro proposto mira a identificare e facilitare programmi di ricerca che miglioreranno la sicurezza e la sicurezza degli australiani attraverso una maggiore consapevolezza, integrazione e coordinamento delle risorse e strategie esistenti per lo sviluppo di politiche, capacità e procedure per la protezione dell'Australia e la lotta contro la criminalità.

Questo Centro riunirà una combinazione unica di ricercatori professionisti e utenti finali nelle discipline forensi; scienze dell'informazione, dati biometrici e comunicazioni sicure; scienze mediche, sanitarie e ambientali; forze dell'ordine, dogane e protezione delle frontiere; e-security, decodifica e frode; governance e gestione del settore pubblico; legge e prove; e modelli sociali ed economici. Il Centro stabilirà collegamenti essenziali e unici tra questi gruppi per consentire ricerche, politiche e sinergie operative che altrimenti non si verificherebbero. Insieme, faranno avanzare la ricerca sulla sicurezza nazionale e internazionale e svilupperanno opzioni innovative per una migliore anticipazione e gestione delle minacce potenzialmente gravi per la salute, il benessere e la sicurezza degli australiani. Ulteriori benefici della ricerca del Centro sono le conseguenze di flusso per la cooperazione cross-portfolio e i risultati che hanno il potenziale per consigliare e guidare il governo e l'industria.

La ricerca sulla sicurezza oggi si concentra sul mantenimento della redditività e sulla protezione dei vari sistemi dalla minaccia e dipende dal contributo di molte discipline che lavorano insieme per garantire che sia in atto un sistema che consideri tutti i fattori rilevanti. La protezione avviene a un certo numero di livelli strategici e comprende la comprensione della natura di una minaccia, la preparazione di una possibile minaccia, la prevenzione delle conseguenze di una minaccia, la risposta alle conseguenze di una minaccia e il recupero dalle conseguenze di una minaccia. Nell'affrontare questi livelli strategici, è essenziale integrare la ricerca che affronta le questioni legali e legali per consentire una risposta e un recupero efficaci. Le minacce alla sicurezza possono essere involontarie, accidentali e non organizzate, oppure possono essere deliberate, organizzate e dannose (incluse attività criminali e terroristiche). Mentre queste ultime minacce occupano molta discussione mediatica, la precedente classe di minacce alla sicurezza può avere conseguenze sociali, ambientali ed economiche devastanti. Ad esempio, si stima che il furto di identità costerebbe all'Australia oltre $ 1,2 miliardi all'anno attraverso carta di credito, conto bancario, sicurezza sociale e altri furti fraudolenti.

Presso l'Università di Canberra abbiamo accademici, personale di ricerca e studenti con l'esperienza, la base di disciplina, risorse, collegamenti collaborativi e profilo di ricerca per raggiungere gli obiettivi di questa Area di forza della ricerca. Questo Centro è unico in quanto è un'area di ricerca multidisciplinare integrata "trasversale all'università", incentrata sulla priorità nazionale di ricerca per la salvaguardia dell'Australia. La nostra posizione di capitale nazionale ci rende anche "naturali" per svolgere un ruolo di collegamento in rete e di collegamento che riunisca ricercatori, professioni, industria e governo per lo sviluppo di programmi e il raggiungimento di risultati che rispondano alle priorità nazionali. Oltre alle nostre risorse specializzate, l'UC possiede anche alcune infrastrutture (ad esempio un'aula di tribunale elettronica, laboratori standard ISO, ecc.) Che ci fornisce un vantaggio competitivo unico rispetto ad altre università

Il Diploma in Lingue è disponibile per tutti gli studenti universitari iscritti all'Università. Questo Diploma consente agli studenti di completare una sequenza di tre anni in una lingua straniera mantenendo i requisiti per la laurea scelta. È disponibile per gli studenti principianti e per gli studenti con precedenti conoscenze della lingua a livello Continuo e Intermedio-Avanzato (vedere voci separate). Gli studenti si iscrivono nella lingua scelta ad ogni livello di anno e normalmente si prevede di iscriversi a meno di un carico a tempo pieno nelle loro unità di laurea per quell'anno. Questo processo estenderà il periodo richiesto per completare entrambi i programmi ad un minimo di quattro anni. Gli studenti possono iniziare le unità di Diploma in primo o secondo anno del loro corso di laurea. Gli studenti che iniziano il Diploma nel loro terzo anno di studio universitario possono laurearsi dalla laurea e terminare le rimanenti unità Diploma a tempo parziale.

Questo Diploma consente agli studenti di sviluppare abilità linguistiche e competenze culturali in cinese, giapponese o spagnolo, aggiungendo valore e internazionalizzando la loro laurea. Gli studenti acquisiranno competenze linguistiche che possono essere utilizzate all'interno delle loro carriere professionali per migliorare le loro opportunità di lavoro in Australia o all'estero. Alla fine del corso al livello iniziale, gli studenti avranno acquisito un livello intermedio di abilità comunicative e competenze culturali nella lingua scelta

Uno studente a tempo pieno normalmente studierà quattro unità in ciascuno dei tre semestri, ma può estendere il corso a due anni studiando tre unità per semestre. Uno studente part-time normalmente studierà due unità per semestre. Nel primo semestre, gli studenti devono seguire la pratica professionale G. Le restanti tre unità possono essere scelte da una varietà di unità a livello di laurea (G) o post-laurea (PG) nelle aree di programmazione, analisi dei sistemi e progettazione, reti di computer e progetto e gestione della qualità. Le unità di livello G verrebbero normalmente selezionate per soddisfare i prerequisiti per le unità di livello PG da prelevare nei semestri 2 e 3.

Nel secondo e nel terzo semestre, gli studenti prendono unità di livello PG nelle aree che li interessano. Le unità di livello PG attualmente in offerta sono elencate di seguito, raggruppate per specializzazione. Le unità con un asterisco (*) hanno il prerequisito di un'esperienza lavorativa di 2 anni. Tieni presente che in qualsiasi anno verrà eseguita solo una selezione di queste unità.
Oltre all'elenco di seguito, unità di progetto individuali e (per studenti specializzati in Business Informatics) sono disponibili uno stage

Gli acronimi di notizie sono acronimi di frasi utilizzate nella scrittura di notizie. Sono per lo più composti delle prime lettere delle parole nella frase e sono le migliori per essere scritte in maiuscolo. Le frases sono generalmente in inglese anche se alcune possono essere in lingua madre.

Il riconoscimento dei pattern viene generalmente classificato in base al tipo di procedura di apprendimento utilizzata per generare il valore di output. L'apprendimento supervisionato presuppone che sia stata fornita una serie di dati di addestramento (il set di allenamento), costituita da un insieme di istanze che sono state correttamente etichettate a mano con l'output corretto. I modelli di allenamento sono gli obiettivi del processo di formazione e non devono essere confusi con il set di allenamento [7]. Una procedura di apprendimento genera quindi un modello che tenta di soddisfare due obiettivi a volte contrastanti: eseguire al meglio i dati di addestramento e generalizzare il più possibile i nuovi dati (in genere, ciò significa essere il più semplici possibile, per alcune definizioni tecniche di "semplice", in accordo con il Rasoio di Occam, discusso di seguito). L'apprendimento senza supervisione, d'altro canto, presuppone che i dati di addestramento non siano stati etichettati con la mano e tenta di trovare modelli intrinseci nei dati che possono essere utilizzati per determinare il valore di output corretto per le nuove istanze di dati. [8] Una combinazione dei due che è stata recentemente esplorata è l'apprendimento semi-supervisionato, che utilizza una combinazione di dati etichettati e non etichettati (in genere una piccola serie di dati etichettati combinati con una grande quantità di dati senza etichetta). Si noti che nei casi di apprendimento non supervisionato, potrebbe non esserci alcun dato di formazione di cui parlare; in altre parole, e i dati da etichettare sono i dati di allenamento.

Si noti che a volte termini diversi vengono usati per descrivere le corrispondenti procedure di apprendimento supervisionate e non supervisionate per lo stesso tipo di output. Ad esempio, l'equivalente non supervisionato della classificazione è normalmente noto come clustering, basato sulla percezione comune dell'attività come non implicante dati di addestramento di cui parlare e di raggruppare i dati di input in cluster basati su alcune misure di similarità intrinseche (ad esempio la distanza tra istanze, considerate come vettori in uno spazio vettoriale multidimensionale), piuttosto che assegnare ciascuna istanza di input a uno di un insieme di classi predefinite. Si noti inoltre che in alcuni campi la terminologia è diversa: ad esempio, nell'ecologia della comunità, il termine "classificazione" è usato per riferirsi a ciò che è comunemente noto come "clustering".

Il pezzo di dati di input per il quale viene generato un valore di output è formalmente definito un'istanza. L'istanza è formalmente descritta da un vettore di caratteristiche, che insieme costituiscono una descrizione di tutte le caratteristiche note dell'istanza. (Questi feature possono essere visti come punti di definizione in uno spazio multidimensionale appropriato, e i metodi per manipolare i vettori in spazi vettoriali possono essere applicati a essi, come il calcolo del prodotto di punti o l'angolo tra due vettori.) Tipicamente, le caratteristiche sono o categoriale (noto anche come nominale, cioè costituito da uno di un insieme di elementi non ordinati, come un genere di "maschio" o "femmina", o un gruppo sanguigno di "A", "B", "AB" o " O "), ordinale (costituito da un insieme di elementi ordinati, ad esempio" grande "," medio "o" piccolo "), con valore intero (ad esempio, un conteggio del numero di occorrenze di una parola particolare in un email) o valori reali (ad esempio, una misurazione della pressione sanguigna). Spesso, i dati categoriali e ordinali sono raggruppati insieme; allo stesso modo per dati con valori interi e valori reali. Inoltre, molti algoritmi funzionano solo in termini di dati categoriali e richiedono che i dati con valori reali o con valori interi siano discretizzati in gruppi (ad esempio, meno di 5, tra 5 e 10 o maggiori di 10).

Molti algoritmi di riconoscimento di pattern comuni sono di natura probabilistica, in quanto utilizzano inferenza statistica per trovare l'etichetta migliore per una determinata istanza. A differenza di altri algoritmi, che emettono semplicemente un'etichetta "migliore", spesso gli algoritmi probabilistici emettono anche una probabilità che l'istanza sia descritta dall'etichetta specificata. Inoltre, molti algoritmi probabilistici producono un elenco delle migliori etichette N con probabilità associate, per un certo valore di N, anziché semplicemente una singola migliore etichetta. Quando il numero di etichette possibili è piuttosto piccolo (ad esempio, nel caso della classificazione), N può essere impostato in modo che venga generata la probabilità di tutte le etichette possibili. Gli algoritmi probabilistici hanno molti vantaggi rispetto agli algoritmi non probabilistici:

Emettono un valore di confidenza associato alla loro scelta. (Si noti che alcuni altri algoritmi possono anche emettere valori di confidenza, ma in generale, solo per gli algoritmi probabilistici questo valore matematicamente radicato nella teoria della probabilità.I valori di confidenza non probabilistici in generale non possono avere alcun significato specifico, e usati solo per confrontarsi con altri valori di confidenza emessi dallo stesso algoritmo).
Corrispondentemente, possono astenersi quando la fiducia di scegliere un particolare risultato è troppo bassa.
A causa dell'output delle probabilità, gli algoritmi probabilistici di riconoscimento del pattern possono essere incorporati in modo più efficace in compiti di apprendimento della macchina più ampi, in modo da evitare parzialmente o completamente il problema della propagazione dell'errore.

Gli algoritmi di selezione delle caratteristiche tentano di eliminare direttamente le funzionalità ridondanti o irrilevanti. È stata fornita un'introduzione generale alla selezione delle caratteristiche che riassume approcci e sfide [9]. La complessità della selezione delle caratteristiche è, a causa del suo carattere non monotono, un problema di ottimizzazione in cui un totale di {\ displaystyle n} n include il powerset costituito da tutti {\ displaystyle 2 ^ {n} -1} 2 ^ { n} È necessario esplorare 1 sottoinsiemi di funzionalità. L'algoritmo Branch-and-Bound [10] riduce questa complessità ma è intrattabile per valori medio-grandi del numero di funzionalità disponibili {\ displaystyle n} n. Per un confronto su larga scala degli algoritmi di selezione delle caratteristiche vedi [11].

Le tecniche per trasformare i vettori di feature grezzi (estrazione di feature) vengono talvolta utilizzate prima dell'applicazione dell'algoritmo di pattern matching. Ad esempio, gli algoritmi di estrazione delle feature tentano di ridurre un vettore di feature di grande dimensionalità in un vettore di dimensioni più piccole che è più semplice da utilizzare e codifica meno ridondanza, utilizzando tecniche matematiche come l'analisi delle componenti principali (PCA). La distinzione tra la selezione delle funzionalità e l'estrazione delle feature è che le funzionalità risultanti dopo l'estrazione delle feature sono di un tipo diverso rispetto alle funzionalità originali e potrebbero non essere facilmente interpretabili, mentre le funzionalità rimaste dopo la selezione delle funzionalità sono semplicemente un sottoinsieme delle funzionalità originali .

L'elaborazione del linguaggio naturale [110] (NLP) offre alle macchine la capacità di leggere e comprendere il linguaggio umano. Un sistema di elaborazione del linguaggio naturale sufficientemente potente consentirebbe interfacce utente in linguaggio naturale e l'acquisizione di conoscenze direttamente da fonti scritte da umani, come i testi di newswire. Alcune semplici applicazioni di elaborazione del linguaggio naturale includono il recupero delle informazioni, il text mining, la risposta alle domande [111] e la traduzione automatica [112]. Molti approcci attuali utilizzano le frequenze di co-occorrenza delle parole per costruire rappresentazioni sintattiche del testo. Le strategie di "individuazione delle parole chiave" per la ricerca sono popolari e scalabili ma stupide; una query di ricerca per "cane" potrebbe corrispondere solo ai documenti con la parola letterale "cane" e perdere un documento con la parola "barboncino". Le strategie di "affinità lessicale" utilizzano il verificarsi di parole come "incidente" per valutare il sentimento di un documento. I moderni approcci statistici alla PNL possono combinare tutte queste strategie e altre, e spesso raggiungere un'accuratezza accettabile a livello di pagina o di paragrafo, ma continuano a mancare la comprensione semantica necessaria per classificare bene le frasi isolate. 

Oltre alle solite difficoltà con la codifica della conoscenza del senso comune del semantico, la PNL semantica esistente a volte scala troppo male per essere valida nelle applicazioni aziendali. Al di là della PNL semantica, l'obiettivo finale della PNL "narrativa" è di incarnare una piena comprensione del ragionamento del senso comune. [113]
La percezione della macchina [114] è la capacità di utilizzare input da sensori (come telecamere (spettro visibile o infrarossi), microfoni, segnali wireless e lidar attivo, sonar, radar e sensori tattili) per dedurre aspetti del mondo. Le applicazioni includono riconoscimento vocale, riconoscimento facciale [115] e riconoscimento di oggetti [116]. La visione artificiale è la capacità di analizzare l'input visivo. Tale input è solitamente ambiguo; un pedone gigante di cinquanta metri di distanza può produrre esattamente gli stessi pixel di un vicino pedone di dimensioni normali, richiedendo all'IA di giudicare la probabilità relativa e la ragionevolezza delle diverse interpretazioni, ad esempio utilizzando il suo "modello oggetto" per valutare quei pedoni di cinquanta metri non esistono. [117]

L'apprendimento automatico, un concetto fondamentale di ricerca sull'intelligenza artificiale sin dal suo inizio, [104] è lo studio di algoritmi informatici che migliorano automaticamente attraverso l'esperienza. [105] [106]

L'apprendimento senza supervisione è la capacità di trovare schemi in un flusso di input, senza richiedere che un essere umano etichetti gli input per primo. [107] L'apprendimento supervisionato include sia la classificazione che la regressione numerica, che richiede che un essere umano etichetti prima i dati di input. La classificazione viene utilizzata per determinare a quale categoria appartiene qualcosa, dopo aver visto un certo numero di esempi di cose da diverse categorie. La regressione è il tentativo di produrre una funzione che descrive la relazione tra input e output e predice come le uscite dovrebbero cambiare quando cambiano gli input. [106] Sia i classificatori che gli studenti di regressione possono essere visti come "approssimatori di funzione" che cercano di apprendere una funzione sconosciuta (possibilmente implicita); ad esempio, un classificatore di spam può essere visto come l'apprendimento di una funzione che mappa dal testo di un'e-mail a una di due categorie, "spam" o "non spam". La teoria dell'apprendimento computazionale può valutare gli studenti in base alla complessità computazionale, alla complessità del campione (la quantità di dati richiesta) o ad altre nozioni di ottimizzazione [108]. Nell'apprendimento rinforzato [109], l'agente viene ricompensato per le buone risposte e punito per quelle cattive. L'agente usa questa sequenza di ricompense e punizioni per formare una strategia per operare nel suo spazio problematico.