Es gibt verschiedene statistische Ansätze zur Spracherkennung, bei denen unterschiedliche Techniken zur Klassifizierung der Daten verwendet werden. Eine Technik besteht darin, die Komprimierbarkeit des Textes mit der Komprimierbarkeit von Texten in einer Reihe bekannter Sprachen zu vergleichen. Dieser Ansatz wird als gegenseitiges auf Informationen basierendes Distanzmaß bezeichnet. Dieselbe Technik kann auch verwendet werden, um Familienstammbäume von Sprachen empirisch zu konstruieren, die den nach historischen Methoden konstruierten Bäumen weitgehend entsprechen. [Zitat erforderlich] Das auf gegenseitigen Informationen basierende Abstandsmaß ist im Wesentlichen äquivalent zu herkömmlicheren modellbasierten Verfahren und wird im Allgemeinen nicht als betrachtet entweder neu oder besser als einfachere Techniken.

Eine andere Technik, wie von Cavnar und Trenkle (1994) und Dunning (1994) beschrieben, besteht darin, ein Sprach-N-Gramm-Modell aus einem "Trainingstext" für jede der Sprachen zu erstellen. Diese Modelle können auf Zeichen (Cavnar und Trenkle) oder auf codierten Bytes (Dunning) basieren. In letzterer sind Spracherkennung und Erkennung der Zeichenkodierung integriert. Dann wird für jedes Textstück, das identifiziert werden muss, ein ähnliches Modell erstellt, und dieses Modell wird mit jedem gespeicherten Sprachmodell verglichen. Die wahrscheinlichste Sprache ist diejenige mit dem Modell, das dem Modell am ähnlichsten ist, da der Text identifiziert werden muss. Dieser Ansatz kann problematisch sein, wenn der eingegebene Text in einer Sprache vorliegt, für die kein Modell vorhanden ist. In diesem Fall gibt die Methode möglicherweise eine andere, "ähnlichste" Sprache als Ergebnis zurück. Problematisch für jeden Ansatz sind auch Teile des Eingabetextes, die aus mehreren Sprachen bestehen, wie dies im Web üblich ist.

Für eine neuere Methode siehe und (2009). Diese Methode kann mehrere Sprachen in einem unstrukturierten Textstück erkennen und arbeitet robust mit kurzen Texten von wenigen Wörtern: Etwas, mit dem das N-Gramm zu kämpfen hat.

Eine ältere statistische Methode von Grefenstette basierte auf der Prävalenz bestimmter Funktionswörter (z. B. "the" in Englisch).

Einer der größten Engpässe bei Spracherkennungssystemen ist die Unterscheidung zwischen eng verwandten Sprachen. Ähnliche Sprachen wie Serbisch und Kroatisch oder Indonesisch und Malaiisch stellen eine signifikante lexikalische und strukturelle Überlappung dar, was es für Systeme schwierig macht, zwischen ihnen zu unterscheiden.

Vor kurzem wurde die DSL-Shared-Task [1] mit einem Datensatz (Tan et al., 2014) organisiert, der 13 verschiedene Sprachen (und Sprachvarianten) in sechs Sprachgruppen umfasst: Gruppe A (Bosnisch, Kroatisch, Serbisch), Gruppe B ( Indonesisch, Malaysisch), Gruppe C (Tschechisch, Slowakisch), Gruppe D (Brasilianisches Portugiesisch, Europäisches Portugiesisch), Gruppe E (Halbinsel Spanien, Argentinisches Spanisch), Gruppe F (Amerikanisches Englisch, Britisches Englisch). Das beste System erzielte eine Leistung von über 95% (Goutte et al., 2014). Ergebnisse der gemeinsam genutzten DSL-Aufgabe werden in Zampieri et al. Beschrieben. 2014

Die Geschichte der Verarbeitung natürlicher Sprache begann im Allgemeinen in den 1950er Jahren, obwohl Arbeiten aus früheren Perioden gefunden werden können. 1950 veröffentlichte Alan Turing einen Artikel mit dem Titel "Intelligence", in dem der sogenannte Turing-Test als Kriterium der Intelligenz vorgeschlagen wurde.

Das Georgetown-Experiment im Jahr 1954 beinhaltete die vollautomatische Übersetzung von mehr als 60 russischen Sätzen ins Englische. Die Autoren behaupteten, dass die maschinelle Übersetzung innerhalb von drei oder fünf Jahren ein gelöstes Problem sein würde. [2] Der reale Fortschritt war jedoch viel langsamer, und nach dem ALPAC-Bericht von 1966, der feststellte, dass die zehnjährige Forschung die Erwartungen nicht erfüllt hatte, wurden die Mittel für die maschinelle Übersetzung drastisch gekürzt. Bis in die späten 1980er Jahre, als die ersten statistischen Maschinenübersetzungssysteme entwickelt wurden, wurde wenig Forschung in der maschinellen Übersetzung durchgeführt.

Einige bemerkenswert erfolgreiche Systeme zur Verarbeitung natürlicher Sprache, die in den 1960er Jahren entwickelt wurden, waren SHRDLU, ein System für natürliche Sprache, das in beschränkten "Blockwelten" mit eingeschränktem Vokabular arbeitet, und ELIZA, eine Simulation eines Rogerianischen Psychotherapeuten, die zwischen 1964 und 1966 von Joseph Weizenbaum geschrieben wurde Keine Informationen über menschliche Gedanken oder Emotionen, ELIZA bot manchmal eine überraschend menschliche Interaktion. Wenn der "Patient" die sehr kleine Wissensbasis überstieg, könnte ELIZA eine generische Antwort geben, beispielsweise auf "Mein Kopf schmerzt" mit "Warum sagen Sie, dass Ihr Kopf schmerzt?".

In den 70er Jahren begannen viele Programmierer, "konzeptuelle Ontologien" zu schreiben, die reale Informationen in vom Computer verständliche Daten strukturierten. Beispiele sind MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979) und Plot Units (Lehnert 1981) ). In dieser Zeit wurden viele Chatterbots geschrieben, darunter PARRY, Racter und Jabberwacky.

Bis in die 1980er Jahre basierten die meisten Systeme für die Verarbeitung natürlicher Sprache auf komplexen handgeschriebenen Regeln. Ab den späten achtziger Jahren gab es jedoch eine Revolution in der Verarbeitung natürlicher Sprachen mit der Einführung maschineller Lernalgorithmen für die Sprachverarbeitung. Dies war sowohl auf die stetige Zunahme der Rechenleistung (siehe Moores Gesetz) als auch auf die allmähliche Abnahme der Dominanz der Chomsky-Theorien der Linguistik (z. B. Transformations-Grammatik) zurückzuführen, deren theoretische Grundlagen die Art der Korpuslinguistik entmutigten, die dem Ansatz des maschinellen Lernens zugrunde liegt zur Sprachverarbeitung. [3] Einige der am frühesten verwendeten Algorithmen für maschinelles Lernen, wie etwa Entscheidungsbäume, erzeugten Systeme mit harten Wenn-dann-Regeln, die den vorhandenen handschriftlichen Regeln ähneln. Durch das Tag-of-Speech-Tagging wurde jedoch die Verwendung von Hidden-Markov-Modellen für die Verarbeitung natürlicher Sprache eingeführt. Zunehmend konzentrierte sich die Forschung auf statistische Modelle, die sanfte, probabilistische Entscheidungen treffen, die auf der Anfügung echter Werte an die Merkmale beruhen, aus denen die Eingabe besteht Daten. Beispiele für solche statistischen Modelle sind die Cache-Sprachmodelle, auf die sich viele Spracherkennungssysteme heute verlassen. Solche Modelle sind im Allgemeinen robuster, wenn sie unbekannte Eingaben erhalten, insbesondere fehlerhafte Eingaben (wie dies für reale Daten sehr üblich ist), und sie liefern zuverlässigere Ergebnisse, wenn sie in ein größeres System mit mehreren Unteraufgaben integriert werden.

Viele der bemerkenswerten frühen Erfolge waren im Bereich der maschinellen Übersetzung zu verzeichnen, insbesondere aufgrund der Arbeit bei IBM Research, wo nach und nach kompliziertere statistische Modelle entwickelt wurden. Diese Systeme konnten die vorhandenen mehrsprachigen Textkorpora nutzen, die vom Parlament von Kanada und der Europäischen Union als Ergebnis von Gesetzen erstellt worden waren, die die Übersetzung aller Regierungsverfahren in alle Amtssprachen der entsprechenden Regierungssysteme forderten. Die meisten anderen Systeme waren jedoch auf Corpora angewiesen, die speziell für die von diesen Systemen ausgeführten Aufgaben entwickelt wurden. Dies war (und bleibt auch weiterhin) eine wesentliche Einschränkung für den Erfolg dieser Systeme. Infolgedessen wurde viel erforscht, wie man aus begrenzten Datenmengen effektiver lernen kann.

Die jüngste Forschung konzentrierte sich zunehmend auf unüberwachte und halbüberwachte Lernalgorithmen. Solche Algorithmen können aus Daten lernen, die nicht manuell mit den gewünschten Antworten kommentiert wurden, oder mithilfe einer Kombination aus kommentierten und nicht kommentierten Daten. Im Allgemeinen ist diese Aufgabe viel schwieriger als das überwachte Lernen und führt in der Regel zu weniger genauen Ergebnissen für eine bestimmte Eingangsdatenmenge. Es gibt jedoch eine enorme Menge nicht annotierter Daten (darunter unter anderem den gesamten Inhalt des World Wide Web), die oft die minderwertigen Ergebnisse ausgleichen können, wenn der verwendete Algorithmus eine ausreichend geringe zeitliche Komplexität aufweist sei praktisch.

In den 2010er Jahren waren Repräsentationslernen und maschinelle Lernmethoden mit tiefen neuronalen Netzwerken im Stil der natürlichen Sprache weit verbreitet, was zum Teil auf eine Vielzahl von Ergebnissen zurückzuführen ist, die zeigen, dass solche Techniken [4] [5] State-of-the-Art-Ergebnisse erzielen können in vielen Aufgaben der natürlichen Sprache, zum Beispiel in der Sprachmodellierung, [6] Parsing, [7] [8] und vielen anderen. Populäre Techniken umfassen die Verwendung von Worteinbettungen zum Erfassen semantischer Eigenschaften von Wörtern und eine Zunahme des End-to-End-Lernens einer übergeordneten Aufgabe (z. B. Beantworten von Fragen), anstatt sich auf eine Pipeline von separaten Zwischenaufgaben (z. B. Teil-of-Speech-Tagging und Analyse von Abhängigkeiten). In einigen Bereichen hat diese Verschiebung wesentliche Änderungen bei der Gestaltung von NLP-Systemen zur Folge, so dass Ansätze auf der Basis eines tiefen neuronalen Netzwerks als ein neues Paradigma betrachtet werden können, das sich von der statistischen Verarbeitung natürlicher Sprache unterscheidet. Zum Beispiel betont der Begriff "Neural Machine Translation" (NMT) die Tatsache, dass tief lernorientierte Ansätze zur maschinellen Übersetzung Sequenz-zu-Sequenz-Transformationen direkt erlernen, so dass keine Zwischenschritte wie Wortausrichtung und Sprachmodellierung erforderlich sind, die in der Statistik verwendet wurden maschinelle Übersetzung (SMT).

In der Anfangszeit wurden viele Sprachverarbeitungssysteme durch manuelles Kodieren eines Satzes von Regeln [9] [10] entworfen, z. indem Sie Grammatiken schreiben oder heuristische Regeln für das Stemming entwickeln. Dies ist jedoch selten robust gegenüber natürlichen Sprachvariationen.

Seit der sogenannten "statistischen Revolution" [11] [12] in den späten 1980er und Mitte der 1990er Jahre stützte sich die Forschung im Bereich der Verarbeitung natürlicher Sprachen stark auf maschinelles Lernen.

Das Machine-Learning-Paradigma fordert stattdessen die Verwendung statistischer Inferenz, um solche Regeln automatisch durch die Analyse von großen Korpora typischer Beispiele aus der realen Welt zu erlernen (ein Korpus (Plural, "Corpora") ist ein Satz von Dokumenten, möglicherweise mit menschlichen oder computergestützten Anmerkungen ).

Viele verschiedene Klassen von Maschinenlernalgorithmen wurden auf Aufgaben der Verarbeitung natürlicher Sprache angewendet. Diese Algorithmen verwenden eine große Anzahl von "Merkmalen", die aus den Eingangsdaten generiert werden. Einige der am frühesten verwendeten Algorithmen, wie z. B. Entscheidungsbäume, erzeugten Systeme mit harten Wenn-dann-Regeln, die den Systemen handschriftlicher Regeln ähnelten, die damals üblich waren. Zunehmend konzentrierte sich die Forschung jedoch auf statistische Modelle, die sanfte, probabilistische Entscheidungen treffen, die auf der Anbringung von realen Gewichtungen an jedes Eingabemerkmal beruhen. Solche Modelle haben den Vorteil, dass sie die relative Sicherheit vieler verschiedener möglicher Antworten ausdrücken können, anstatt nur einer. Dadurch werden zuverlässigere Ergebnisse erzielt, wenn ein solches Modell als Bestandteil eines größeren Systems enthalten ist.

Systeme, die auf Machine-Learning-Algorithmen basieren, bieten gegenüber handgefertigten Regeln viele Vorteile:

Die Lernverfahren, die während des maschinellen Lernens verwendet werden, konzentrieren sich automatisch auf die häufigsten Fälle, während beim Schreiben von Regeln mit der Hand oft nicht klar ist, wohin die Anstrengung gerichtet werden soll.

Automatische Lernprozeduren können statistische Inferenzalgorithmen verwenden, um Modelle zu erzeugen, die robust gegenüber unbekannten Eingaben sind (z. B. Wörter oder Strukturen enthalten, die zuvor noch nicht gesehen wurden) und zu fehlerhaften Eingaben (z. B. mit falsch geschriebenen oder versehentlich ausgelassenen Wörtern). Im Allgemeinen ist es äußerst schwierig, fehleranfällig und zeitaufwändig, solche Eingaben mit handschriftlichen Regeln ordnungsgemäß zu handhaben - oder generell, Systeme handgeschriebener Regeln zu schaffen, die weiche Entscheidungen treffen.

Systeme, die auf dem automatischen Erlernen der Regeln basieren, können einfach durch die Bereitstellung von mehr Eingabedaten genauer gemacht werden. Systeme, die auf handschriftlichen Regeln basieren, können jedoch nur durch Erhöhung der Komplexität der Regeln genauer gemacht werden, was eine viel schwierigere Aufgabe ist. Insbesondere ist die Komplexität von Systemen, die auf handwerklichen Regeln basieren, begrenzt, und darüber hinaus werden die Systeme immer unüberschaubarer. Das Erstellen von mehr Daten für die Eingabe in maschinelle Lernsysteme erfordert jedoch einfach eine entsprechende Erhöhung der Anzahl der geleisteten Arbeitsstunden, im Allgemeinen ohne wesentliche Erhöhung der Komplexität des Anmerkungsprozesses.

Die Erkennung der Zeichenkodierung, die Erkennung von Zeichensätzen oder die Erkennung von Codeseiten ist das heuristische Erraten der Zeichenkodierung einer Reihe von Bytes, die Text darstellen. Die Technik gilt als unzuverlässig und wird nur verwendet, wenn bestimmte Metadaten wie ein HTTP-Content-Type-Header entweder nicht verfügbar sind oder als nicht vertrauenswürdig angesehen werden.

Dieser Algorithmus beinhaltet normalerweise eine statistische Analyse von Byte-Mustern, wie z. B. die Häufigkeitsverteilung von Trigraphen verschiedener Sprachen, die in jeder erkannten Codepage codiert sind. Eine solche statistische Analyse kann auch zur Spracherkennung verwendet werden. Dieser Prozess ist nicht narrensicher, da er von statistischen Daten abhängig ist.

Im Allgemeinen führt eine fehlerhafte Zeichensatzerkennung zu Mojibake.

In einem der wenigen Fälle, in denen die Erkennung von Zeichensätzen zuverlässig funktioniert, wird UTF-8 erkannt. Dies ist auf den hohen Prozentsatz ungültiger Byte-Sequenzen in UTF-8 zurückzuführen, so dass Text in einer anderen Kodierung, die Byte mit gesetztem High-Bit verwendet, äußerst unwahrscheinlich ist, einen UTF-8-Gültigkeitstest zu bestehen. Schlecht geschriebene Zeichensatzerkennungsroutinen führen den zuverlässigen UTF-8-Test jedoch nicht zuerst aus und entscheiden möglicherweise, dass UTF-8 eine andere Codierung ist. Es war zum Beispiel üblich, dass in UTF-8 Websites mit dem Namen der deutschen Stadt München als München angezeigt wurden.

UTF-16 ist aufgrund der hohen Anzahl von Zeilenumbrüchen (U + 000A) und Leerzeichen (U + 0020), die beim Aufteilen der Daten in 16-Bit-Wörter gefunden werden sollten, und der Tatsache, dass nur wenige Kodierungen 16- kleine Worte. Dieser Prozess ist nicht narrensicher; Beispielsweise würden einige Versionen des Windows-Betriebssystems den Ausdruck "Bush hat die Fakten" (ohne Zeilenumbruch) in ASCII als chinesisches UTF-16LE falsch erkannt.

Die Erkennung von Zeichensätzen ist in Europa in einer Umgebung von gemischten ISO-8859-Kodierungen besonders unzuverlässig. Dies sind eng verwandte Acht-Bit-Kodierungen, die sich in ihrer unteren Hälfte mit ASCII überlappen. Es gibt keine technische Möglichkeit, diese Kodierungen voneinander zu unterscheiden, und die Erkennung beruht auf der Identifizierung von Sprachmerkmalen wie Buchstabenhäufigkeiten oder Schreibweisen.

Aufgrund der Unzuverlässigkeit der heuristischen Erkennung ist es besser, Datensätze ordnungsgemäß mit der richtigen Kodierung zu kennzeichnen. Für HTML-Dokumente, die über das Web über das Web bereitgestellt werden, sollte die Codierung außerhalb des Bandes mit dem Header Content-Type: angegeben werden.

Dieser Bereich wird einen Schwerpunkt für Forschung und Innovation in Bereichen, die sich auf Sicherheitssysteme und forensische Untersuchungen beziehen, bereitstellen und wird auf der nationalen und internationalen Stellung des vorhandenen Fachwissens in diesen Bereichen an der Universität von Canberra aufbauen. Es bringt Schlüsselforscher aus allen drei Abteilungen der Universität zusammen, verbindet sich mit bestehenden Forschungsfeldern der verschiedenen Disziplinen an der Universität von Canberra und arbeitet mit externen Organisationen zusammen, die sich auf dieses Gebiet beziehen. Dieses Zentrum wird sich mit wichtigen Themen befassen und die potenziellen Ergebnisse in diesem wichtigen Bereich erweitern, indem es eine multidisziplinäre Gruppe von Forschern und Fachleuten zusammenbringt, die eng mit Regierung, Industrie und Berufen zusammenarbeiten, um die angegebenen Ergebnisse zu erzielen

Ein national und international anerkanntes Exzellenzzentrum in der Forschung zu sein, das proaktiv umfassende und integrierte multidisziplinäre Lösungen für Fragen der Sicherheits- und Forensik bereitstellt und innovative Kollaborationen mit Industrie und Fachleuten anlockt, deren Ergebnisse unmittelbar von Nutzen sind die nationalen und internationalen Gemeinschaften

Entwicklung eines umfassenden Forschungsansatzes, der auf die Verbesserung der Sicherheit in einer Reihe von Bereichen abzielt, die neuartige, integrierte und synergistische Ergebnisse in den Bereichen Informationswissenschaften, Kryptologie, elektronische Systeme und elektronische Kriminalität, Grenzschutz und Zoll sowie biologische Systeme haben werden; einschließlich der Umwelt, der Gesundheit von Mensch und Tier.
Ermittlung und Koordinierung innovativer Forschungsprogramme sowohl in Sicherheits- als auch in forensischen Bereichen, die den Industriesektor einbeziehen und synergistische, branchengebundene Forschungskooperationen und Postgraduiertenstudienprogramme bieten;
Proaktiv bei der Ermittlung von Bereichen zu sein, die eine strategische Weiterentwicklung des Wissens und der Lösungen für neue und bestehende Sicherheitsbedrohungen und forensische Bewertungen erfordern, und die Ergebnisse für die Umsetzung in den relevanten öffentlichen oder privaten Wirtschaftssektoren zu verbreiten; und
Durch aktives Engagement der Rechts- und Politikspezialisten tatsächliche und zukünftige Vorteile in den Bereichen Sicherheit und Forensik zu erreichen, die zu einer Verringerung der Kriminalität führen, durch verbesserte Beweisanalyse für Verurteilungs- und Präventionssysteme.

Diese Ziele werden an den Ergebnissen und Ergebnissen gemessen, die sich aus der Forschung und ihrer Nutzung in Industrie und Beruf ergeben, deren Einfluss auf die Gesellschaft, ihre Einflussnahme auf die Politik und der messbare Nutzen, den sie für die Strafverfolgung und das rechtliche Verfahren bringen

In den letzten zehn Jahren haben sich sowohl unsere nationalen Sicherheitsanforderungen als auch die Muster menschlicher Konflikte grundlegend verändert. Daher fordern die Regierungen integrierte, koordinierte und kosteneffiziente Lösungen, um auf ein breiteres Bedrohungsspektrum zu reagieren. Es besteht eindeutig ein Bedarf an ähnlichen Sicherheits- und forensischen Forschungserleichterungen in Australien. Dieses Zentrum ist insofern einzigartig, als es für eine umfassendere interdisziplinäre Analyse der Sicherheits- und Forensikbedürfnisse Australiens gedacht ist, und es verfolgt einen globaleren strategischen Ansatz für die wichtigen Fragen. Das vorgeschlagene Zentrum zielt darauf ab, Forschungsprogramme zu ermitteln und zu erleichtern, die die Sicherheit und Sicherheit der Australier verbessern, indem das Bewusstsein, die Integration und die Koordinierung der vorhandenen Ressourcen und Strategien für die Entwicklung von Politiken, Fähigkeiten und Verfahren zum Schutz und zur Kriminalitätsbekämpfung in Australien verbessert werden.

Dieses Zentrum wird eine einzigartige Kombination von professionellen Forschern und Endbenutzern in den forensischen Disziplinen zusammenbringen. Informationswissenschaften, Biometrie und sichere Kommunikation; Medizin-, Gesundheits- und Umweltwissenschaften; Strafverfolgung, Zoll- und Grenzschutz; elektronische Sicherheit, Entschlüsselung und Betrug; Governance und Management im öffentlichen Sektor; Gesetz und Beweismittel; und soziales und wirtschaftliches Modellieren. Das Zentrum wird wesentliche und einzigartige Verbindungen zwischen diesen Gruppen herstellen, um Forschung, Politik und betriebliche Synergien zu ermöglichen, die sonst nicht auftreten würden. Gemeinsam werden sie die nationale und internationale Sicherheitsforschung vorantreiben und innovative Optionen entwickeln, um potenziell ernsthafte Bedrohungen für die Gesundheit, das Wohlbefinden und die Sicherheit der Australier besser zu antizipieren und zu bewältigen. Weitere Vorteile der Forschung des Zentrums sind fließende Konsequenzen für die bereichsübergreifende Zusammenarbeit und Ergebnisse, die das Potenzial haben, Regierung und Industrie zu beraten und zu lenken.

Die Sicherheitsforschung konzentriert sich heute auf die Aufrechterhaltung der Tragfähigkeit und den Schutz verschiedener Systeme vor Bedrohungen. Sie hängt vom Beitrag vieler Disziplinen ab, die zusammenarbeiten, um sicherzustellen, dass ein System vorhanden ist, das alle relevanten Faktoren berücksichtigt. Schutz findet auf verschiedenen strategischen Ebenen statt und umfasst das Verständnis der Art einer Bedrohung, die Vorbereitung auf eine mögliche Bedrohung, die Verhinderung der Folgen einer Bedrohung, die Reaktion auf die Folgen einer Bedrohung und die Erholung von den Folgen einer Bedrohung. Bei der Bewältigung dieser strategischen Ebenen ist es unerlässlich, die Forschung zu forensischen und rechtlichen Fragen zu integrieren, um eine wirksame Reaktion und Erholung zu ermöglichen. Sicherheitsbedrohungen können unbeabsichtigt, versehentlich und nicht organisiert sein, oder sie können vorsätzlich, organisiert und böswillig sein (einschließlich krimineller und terroristischer Aktivitäten). Während die letzteren Bedrohungen derzeit viel in den Medien diskutiert werden, kann die frühere Klasse von Sicherheitsbedrohungen erhebliche verheerende soziale, ökologische und wirtschaftliche Folgen haben. Als Beispiel wird nun geschätzt, dass Identitätsdiebstahl Australien durch Kreditkarten, Bankkonten, Sozialversicherungen und andere betrügerische Diebstahl jährlich über 1,2 Milliarden Dollar kostet.

An der Universität von Canberra haben wir Wissenschaftler, Forscher und Studenten mit Fachwissen, Disziplin-Basis, Ressourcen, kooperativen Verbindungen und Forschungsprofil, um die Ziele dieses Forschungsbereichs zu erreichen. Dieses Zentrum ist insofern einzigartig, als es ein integrierter, universitätsübergreifender, multidisziplinärer Bereich der Forschungsstärke ist, der sich auf die nationale Forschungspriorität von Safeguarding Australia konzentriert. Unser nationaler Hauptstadtstandort macht uns auch zu einer "natürlichen" Rolle, um eine Netzwerk- und Verknüpfungsrolle zu spielen, bei der Forscher, Berufe, Industrie und Regierung zusammenkommen, um Programme zu entwickeln und Ergebnisse zu erzielen, die den nationalen Prioritäten entsprechen. Neben unseren spezialisierten Ressourcen verfügt UC auch über eine bestimmte Infrastruktur (z. B. elektronische Gerichtssäle, ISO-Normlaboratorien usw.), die uns einen einzigartigen Wettbewerbsvorteil gegenüber anderen Universitäten bietet

Das Diplom in Sprachen steht allen eingeschriebenen Studenten der Universität zur Verfügung. Dieses Diplom ermöglicht es den Studierenden, eine dreijährige Sequenz in einer Fremdsprache zu absolvieren und gleichzeitig die Anforderungen für den gewählten Abschluss zu wahren. Es ist für Anfänger und für Schüler mit Vorkenntnissen der Sprache in den Stufen Fortgeschritten und Mittelstufe-Fortgeschrittene verfügbar (siehe separate Einträge). Die Studierenden melden sich für jedes Jahr in der von ihnen gewählten Sprache an und es wird normalerweise erwartet, dass sie sich in ihren Studieneinheiten in diesem Jahr in weniger als einer Vollzeitbelastung anmelden. Durch diesen Prozess wird der für die Durchführung beider Programme erforderliche Zeitraum auf mindestens vier Jahre verlängert. Studenten können die Diplomeinheiten im ersten oder zweiten Jahr ihres Bachelor-Studiums beginnen. Studierende, die das Diplom im dritten Jahr ihres Grundstudiums beginnen, können ihren Abschluss machen und die verbleibenden Diploma-Einheiten in Teilzeit abschließen.

Dieses Diplom ermöglicht den Studierenden, Sprachkenntnisse und kulturelle Kompetenz in Chinesisch, Japanisch oder Spanisch zu entwickeln, um ihren Abschluss zu verbessern und zu internationalisieren. Die Studierenden erwerben Sprachkenntnisse, die in ihrer beruflichen Laufbahn genutzt werden können, um ihre Beschäftigungsmöglichkeiten in Australien oder im Ausland zu verbessern. Am Ende des Kurses auf der Grundstufe haben die Studierenden kommunikative Fähigkeiten und kulturelle Kompetenzen in ihrer gewählten Sprache erworben

Ein Vollzeitstudent würde normalerweise vier Einheiten in jedem der drei Semester studieren, kann jedoch den Kurs auf zwei Jahre verlängern, indem er drei Einheiten pro Semester studiert. Ein Teilzeitstudent lernt normalerweise zwei Einheiten pro Semester. Im ersten Semester müssen die Studierenden die Berufspraxis G ablegen. Die verbleibenden drei Einheiten können aus einer Vielzahl von Einheiten auf Graduierten- (G) oder Postgraduierten-Niveau (PG) in den Bereichen Programmierung, Systemanalyse und -design, Computernetze und Projekte ausgewählt werden und Qualitätsmanagement. G-Level-Einheiten werden normalerweise so ausgewählt, dass sie alle Voraussetzungen erfüllen, damit PG-Level-Einheiten in den Semestern 2 und 3 belegt werden können.

Im zweiten und dritten Semester absolvieren die Studierenden Einheiten auf PG-Ebene in für sie interessanten Bereichen. Die derzeit angebotenen Einheiten auf PG-Ebene sind nach Spezialisierung gruppiert. Einheiten mit einem Sternchen (*) setzen eine 2-jährige Berufserfahrung voraus. Bitte beachten Sie, dass in einem Jahr nur eine Auswahl dieser Einheiten ausgeführt wird.
Neben der folgenden Liste stehen einzelne Projekteinheiten und (für Studenten der Fachrichtung Wirtschaftsinformatik) ein Praktikum zur Verfügung

Nachrichtenakronyme sind Akronyme von Frasen, die beim Schreiben von Nachrichten verwendet werden. Sie bestehen meistens aus den Anfangsbuchstaben der Wörter im Wortlaut und sollten am besten in Großbuchstaben geschrieben werden. Die Frases sind im Allgemeinen in Englisch, obwohl einige in Muttersprachen sind.

Die Mustererkennung wird im Allgemeinen nach der Art der Lernprozedur kategorisiert, die zur Erzeugung des Ausgabewerts verwendet wird. Beim überwachten Lernen wird davon ausgegangen, dass ein Satz von Trainingsdaten (der Trainingssatz) bereitgestellt wurde, der aus einem Satz von Instanzen besteht, die ordnungsgemäß von Hand mit der richtigen Ausgabe gekennzeichnet wurden. Trainingsmuster sind die Ziele des Trainingsprozesses und dürfen nicht mit dem Trainingssatz verwechselt werden. [7] Eine Lernprozedur generiert dann ein Modell, das versucht, zwei manchmal widersprüchliche Ziele zu erreichen: Führen Sie die Trainingsdaten so gut wie möglich durch und verallgemeinern Sie die neuen Daten so gut wie möglich (normalerweise bedeutet dies für einige technische Definitionen so einfach wie möglich) von "einfach", in Übereinstimmung mit dem Rasiermesser von Occam, unten diskutiert). Beim unbeaufsichtigten Lernen werden dagegen Trainingsdaten angenommen, die nicht von Hand beschriftet wurden, und es wird versucht, inhärente Muster in den Daten zu finden, die dann zur Bestimmung des korrekten Ausgabewerts für neue Dateninstanzen verwendet werden können. [8] Eine Kombination der beiden, die kürzlich erforscht wurde, ist das semi-beaufsichtigte Lernen, bei dem eine Kombination aus markierten und nicht markierten Daten verwendet wird (normalerweise ein kleiner Satz von markierten Daten in Kombination mit einer großen Menge unmarkierter Daten). Beachten Sie, dass bei unbeaufsichtigtem Lernen möglicherweise überhaupt keine Trainingsdaten vorliegen; mit anderen Worten, und die zu kennzeichnenden Daten sind die Trainingsdaten.

Beachten Sie, dass manchmal unterschiedliche Begriffe verwendet werden, um die entsprechenden überwachten und nicht überwachten Lernverfahren für dieselbe Art von Ausgabe zu beschreiben. Das unüberwachte Äquivalent der Klassifizierung wird zum Beispiel normalerweise als Clustering bezeichnet, das auf der üblichen Wahrnehmung der Aufgabe basiert, ohne dass Trainingsdaten zum Einsatz kommen, und dass die Eingabedaten auf Grundlage eines inhärenten Ähnlichkeitsmaßes (z. B. der Entfernung zwischen) in Cluster gruppiert werden Instanzen, die als Vektoren in einem mehrdimensionalen Vektorraum betrachtet werden), anstatt jeder Eingabeinstanz einer von vordefinierten Klassen zuzuordnen. Beachten Sie auch, dass in einigen Bereichen die Terminologie unterschiedlich ist: In der Ökologie der Gemeinschaft wird der Begriff "Klassifizierung" verwendet, um auf das zu verweisen, was allgemein als "Clustering" bezeichnet wird.

Die Eingabedaten, für die ein Ausgabewert generiert wird, werden formal als Instanz bezeichnet. Die Instanz wird formal durch einen Merkmalsvektor beschrieben, der zusammen eine Beschreibung aller bekannten Eigenschaften der Instanz darstellt. (Diese Merkmalsvektoren können als definierende Punkte in einem geeigneten mehrdimensionalen Raum angesehen werden, und Verfahren zum Manipulieren von Vektoren in Vektorräumen können entsprechend auf sie angewendet werden, z. B. das Berechnen des Punktprodukts oder des Winkels zwischen zwei Vektoren.) Typischerweise gibt es Merkmale kategorial (auch bekannt als nominal, dh bestehend aus einem Satz ungeordneter Elemente, z. B. Geschlecht "männlich" oder "weiblich") oder Blutgruppe "A", "B", "AB" oder " O "), Ordinalzahl (bestehend aus einem Satz geordneter Elemente, z. B." groß "," mittel "oder" klein "), ganzzahlig (z. B. Anzahl der Vorkommen eines bestimmten Wortes in einem E-Mail) oder reelle Werte (z. B. Blutdruckmessung). Oft werden kategoriale und ordinale Daten zusammen gruppiert. ebenso für ganzzahlige und reelle Daten. Darüber hinaus funktionieren viele Algorithmen nur im Hinblick auf kategoriale Daten und erfordern, dass Daten mit echtem oder ganzzahligem Wert in Gruppen diskretisiert werden (z. B. weniger als 5, zwischen 5 und 10 oder mehr als 10).

Viele gängige Mustererkennungsalgorithmen haben einen probabilistischen Charakter, indem sie statistische Inferenz verwenden, um das beste Label für eine bestimmte Instanz zu finden. Im Gegensatz zu anderen Algorithmen, die einfach ein "bestes" Label ausgeben, geben Wahrscheinlichkeitsalgorithmen oft auch eine Wahrscheinlichkeit aus, dass die Instanz durch das angegebene Label beschrieben wird. Darüber hinaus geben viele probabilistische Algorithmen eine Liste der N-besten Labels mit zugehörigen Wahrscheinlichkeiten für einen bestimmten Wert von N aus, anstatt nur ein einzelnes bestes Label. Wenn die Anzahl möglicher Markierungen ziemlich klein ist (z. B. im Fall einer Klassifizierung), kann N so eingestellt werden, dass die Wahrscheinlichkeit aller möglichen Markierungen ausgegeben wird. Probabilistische Algorithmen bieten gegenüber nicht-probabilistischen Algorithmen viele Vorteile:

Sie geben einen mit ihrer Wahl verknüpften Vertrauenswert aus. (Beachten Sie, dass einige andere Algorithmen auch Konfidenzwerte ausgeben können, aber im Allgemeinen ist dieser Wert nur für probabilistische Algorithmen mathematisch in der Wahrscheinlichkeitstheorie begründet. Nicht-probabilistische Konfidenzwerte können im Allgemeinen keine spezifische Bedeutung erhalten und werden nur zum Vergleich verwendet andere Konfidenzwerte, die von demselben Algorithmus ausgegeben werden.)
Entsprechend können sie verzichten, wenn das Vertrauen in die Wahl eines bestimmten Outputs zu gering ist.
Aufgrund der Wahrscheinlichkeitsausgabe können probabilistische Mustererkennungsalgorithmen effektiver in größere maschinelle Lernaufgaben integriert werden, so dass das Problem der Fehlerausbreitung teilweise oder vollständig vermieden wird.

Algorithmen zur Auswahl von Features versuchen, redundante oder irrelevante Features direkt auszuschließen. Es wurde eine allgemeine Einführung in die Merkmalsauswahl gegeben, die Ansätze und Herausforderungen zusammenfasst. [9] Die Komplexität der Merkmalsauswahl ist aufgrund ihres nicht monotonen Charakters ein Optimierungsproblem, bei dem bei gegebenen {{displaystyle n} n das Powerset aus allen {\ displaystyle 2 ^ {n} -1} 2} {besteht. n} -1 Teilmengen von Funktionen müssen untersucht werden. Der Branch-and-Bound-Algorithmus [10] reduziert diese Komplexität, ist jedoch für mittlere bis große Werte der Anzahl verfügbarer Features {\ displaystyle n} n unpraktisch. Einen umfassenden Vergleich von Algorithmen zur Auswahl von Merkmalen finden Sie hier. [11]

Techniken zum Transformieren der Rohmerkmalsvektoren (Merkmalsextraktion) werden manchmal vor der Anwendung des Musteranpassungsalgorithmus verwendet. Beispielsweise versuchen Algorithmen zum Extrahieren von Merkmalen, einen Merkmalsvektor mit großer Dimension in einen Vektor mit geringerer Dimension zu reduzieren, der einfacher zu verarbeiten ist und weniger Redundanz codiert, wobei mathematische Techniken wie die Hauptkomponentenanalyse (Main Component Analysis, PCA) verwendet werden. Der Unterschied zwischen Feature-Auswahl und Feature-Extraktion besteht darin, dass die resultierenden Features nach der Feature-Extraktion anders als die ursprünglichen Features sind und möglicherweise nicht leicht interpretierbar sind, während die nach der Feature-Auswahl verbleibenden Features lediglich eine Teilmenge der ursprünglichen Features sind .

Natural Language Processing [110] (NLP) gibt Maschinen die Fähigkeit, menschliche Sprache zu lesen und zu verstehen. Ein ausreichend leistungsfähiges System für die Verarbeitung natürlicher Sprache würde Benutzeroberflächen in natürlicher Sprache und den Erwerb von Wissen direkt aus von Menschen geschriebenen Quellen, wie z. B. Nachrichtentexten, ermöglichen. Einige unkomplizierte Anwendungen der Verarbeitung natürlicher Sprache umfassen das Abrufen von Informationen, das Text-Mining, das Beantworten von Fragen [111] und die maschinelle Übersetzung. [112] Viele derzeitige Ansätze verwenden Worthäufigkeit von Wörtern, um syntaktische Darstellungen von Text zu erstellen. "Keyword-Spotting" -Strategien für die Suche sind beliebt und skalierbar, aber dumm. Eine Suchanfrage nach "Hund" kann nur Dokumente mit dem Wort "Hund" abgleichen und ein Dokument mit dem Wort "Pudel" verpassen. "Lexikalische Affinitätsstrategien" verwenden das Auftreten von Wörtern wie "Unfall", um das Gefühl eines Dokuments zu bewerten. Moderne statistische NLP-Ansätze können alle diese Strategien und andere kombinieren und oft eine akzeptable Genauigkeit auf Seiten- oder Absatzebene erzielen, es fehlt jedoch immer noch das semantische Verständnis, das zur Klassifizierung einzelner Sätze erforderlich ist. 

Abgesehen von den üblichen Schwierigkeiten bei der Kodierung von Wissen über semantisches Allgemeinwissen, ist das vorhandene semantische NLP manchmal zu schlecht skalierbar, um für geschäftliche Anwendungen geeignet zu sein. Jenseits des semantischen NLP besteht das Endziel des "narrativen" NLP darin, ein umfassendes Verständnis der Vernunft zu begründen. [113]
Maschinenwahrnehmung [114] ist die Fähigkeit, Eingaben von Sensoren (wie Kameras (sichtbares Spektrum oder Infrarot), Mikrofone, drahtlose Signale und aktive Lidar-, Sonar-, Radar- und Berührungssensoren) zu verwenden, um Aspekte der Welt abzuleiten. Anwendungen umfassen Spracherkennung [115], Gesichtserkennung und Objekterkennung. [116] Computer Vision ist die Fähigkeit, visuelle Eingaben zu analysieren. Eine solche Eingabe ist in der Regel mehrdeutig. Ein riesiger, fünfzig Meter großer Fußgänger in der Ferne erzeugt möglicherweise genau dieselben Pixel wie ein normaler Fußgänger in der Nähe, so dass die KI die relative Wahrscheinlichkeit und Angemessenheit der verschiedenen Interpretationen beurteilen muss, beispielsweise anhand ihres "Objektmodells" zur Beurteilung dass fünfzig Meter Fußgänger nicht existieren. [117]

Maschinelles Lernen, ein grundlegendes Konzept der KI-Forschung seit seiner Einführung [104], ist das Studium von Computeralgorithmen, die sich durch Erfahrung automatisch verbessern. [105] [106]

Unüberwachtes Lernen ist die Fähigkeit, Muster in einem Eingabestrom zu finden, ohne dass ein Mensch die Eingaben zuerst benennen muss. [107] Überwachtes Lernen umfasst sowohl die Klassifizierung als auch die numerische Regression. Dies erfordert, dass ein Mensch die Eingabedaten zuerst benennt. Klassifizierung wird verwendet, um zu bestimmen, in welche Kategorie etwas gehört, nachdem mehrere Beispiele aus verschiedenen Kategorien angezeigt wurden. Regression ist der Versuch, eine Funktion zu erzeugen, die die Beziehung zwischen Ein- und Ausgängen beschreibt und vorhersagt, wie sich die Ausgänge ändern sollen, wenn sich die Eingänge ändern. [106] Sowohl Klassifizierer als auch Regressionslerner können als "Funktionsapproximatoren" betrachtet werden, die versuchen, eine unbekannte (möglicherweise implizite) Funktion zu lernen. Ein Spam-Klassifizierer kann beispielsweise als Lernen einer Funktion betrachtet werden, die aus dem Text einer E-Mail eine der beiden Kategorien "Spam" oder "Nicht Spam" abbildet. Die Theorie des rechnergestützten Lernens kann die Lernenden anhand der rechnerischen Komplexität, anhand der Stichproben-Komplexität (wie viele Daten sind erforderlich) oder anhand anderer Optimierungsvorstellungen beurteilen. [108] Beim Verstärkungslernen [109] wird der Agent für gute Antworten belohnt und für schlechte bestraft. Der Agent verwendet diese Abfolge von Belohnungen und Strafen, um eine Strategie für das Arbeiten in seinem Problembereich zu bilden.